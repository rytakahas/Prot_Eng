{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-06-09T14:35:50.532069Z",
     "iopub.status.busy": "2025-06-09T14:35:50.531315Z",
     "iopub.status.idle": "2025-06-09T14:37:08.547962Z",
     "shell.execute_reply": "2025-06-09T14:37:08.547053Z",
     "shell.execute_reply.started": "2025-06-09T14:35:50.532042Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Install necessary packages\n",
    "!pip install torch transformers scikit-learn pandas numpy biopython peft bitsandbytes requests optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-09T14:37:50.114518Z",
     "iopub.status.busy": "2025-06-09T14:37:50.114282Z",
     "iopub.status.idle": "2025-06-09T14:37:58.907413Z",
     "shell.execute_reply": "2025-06-09T14:37:58.906597Z",
     "shell.execute_reply.started": "2025-06-09T14:37:50.114495Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Install necessary tools in Google Colab\n",
    "!apt-get install -y mafft\n",
    "!apt-get install -y hmmer\n",
    "!wget https://github.com/soedinglab/MMseqs2/releases/download/17-b804f/mmseqs-linux-gpu.tar.gz\n",
    "!tar xvf mmseqs-linux-gpu.tar.gz\n",
    "!chmod +x mmseqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-09T14:37:58.908504Z",
     "iopub.status.busy": "2025-06-09T14:37:58.908285Z",
     "iopub.status.idle": "2025-06-09T14:38:27.430872Z",
     "shell.execute_reply": "2025-06-09T14:38:27.430318Z",
     "shell.execute_reply.started": "2025-06-09T14:37:58.908480Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import requests\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, Trainer, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "from Bio import SeqIO\n",
    "from Bio import AlignIO\n",
    "import optuna\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import time\n",
    "import xml.etree.ElementTree as ET\n",
    "from scipy.stats import pearsonr\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-09T14:40:43.073412Z",
     "iopub.status.busy": "2025-06-09T14:40:43.072355Z",
     "iopub.status.idle": "2025-06-09T14:41:53.435114Z",
     "shell.execute_reply": "2025-06-09T14:41:53.434520Z",
     "shell.execute_reply.started": "2025-06-09T14:40:43.073384Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import requests, time, subprocess\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "from Bio import AlignIO\n",
    "\n",
    "WT_SEQUENCE = \"MRHGDISSSNDTVGVAVVNYKMPRLHTAAEVLDNARKIAEMIVGMKQGLPGMDLVVFPEYSLQGIMYDPAEMMETAVAIPGEETEIFSRACRKANVWGVFSLTGERHEEHPRKAPYNTLVLIDNNGEIVQKYRKIIPWCPIEGWYPGGQTYVSEGPKGMKISLIICDDGNYPEIWRDCAMKGAELIVRCQGYMYPAKDQQVMMAKAMAWANNCYVAVANAAGFDGVYSYFGHSAIIGFDGRTLGECGEEEMGIQYAQLSLSQIRDARANDQSQNHLFKILHRGYSGLQASGDGDRGLAECPFEFYRTWVTDAEKARENVERLTRSTTGVAQCPVGRLPYEG\"\n",
    "BLAST_URL = \"https://blast.ncbi.nlm.nih.gov/Blast.cgi\"\n",
    "\n",
    "# ======= Step 1: BLAST API for Homologs =======\n",
    "def run_blast_search(wt_sequence, identity_threshold=90.0, max_retries=30, sleep_time=10, min_length=100, hitlist_size=300):\n",
    "    \"\"\"Run BLAST and extract homologous sequences below a given identity threshold.\"\"\"\n",
    "    params = {\n",
    "        \"CMD\": \"Put\",\n",
    "        \"PROGRAM\": \"blastp\",\n",
    "        \"DATABASE\": \"nr\",\n",
    "        \"QUERY\": wt_sequence,\n",
    "        \"FORMAT_TYPE\": \"XML\",\n",
    "        \"EXPECT\": \"1e-2\",\n",
    "        \"HITLIST_SIZE\": str(hitlist_size)\n",
    "    }\n",
    "    response = requests.post(BLAST_URL, data=params)\n",
    "    response.raise_for_status()\n",
    "    response_text = response.text\n",
    "    if \"RID = \" not in response_text:\n",
    "        raise Exception(\"No RID found in BLAST response.\")\n",
    "    rid = response_text.split(\"RID = \")[-1].split(\"\\n\")[0].strip()\n",
    "    print(f\"BLAST RID: {rid}\")\n",
    "\n",
    "    # Wait for completion\n",
    "    for attempt in range(max_retries):\n",
    "        status = requests.get(BLAST_URL, params={\"CMD\":\"Get\", \"FORMAT_OBJECT\":\"SearchInfo\", \"RID\":rid})\n",
    "        if \"Status=READY\" in status.text:\n",
    "            print(\"BLAST complete.\")\n",
    "            break\n",
    "        print(f\"Waiting... {attempt+1}/{max_retries}\")\n",
    "        time.sleep(sleep_time)\n",
    "    else:\n",
    "        raise Exception(\"BLAST timed out\")\n",
    "\n",
    "    # Download results\n",
    "    result = requests.get(BLAST_URL, params={\"CMD\":\"Get\", \"FORMAT_TYPE\":\"XML\", \"RID\":rid})\n",
    "    result.raise_for_status()\n",
    "    root = ET.fromstring(result.text)\n",
    "    seqs = []\n",
    "    for hit in root.findall(\".//Hit\"):\n",
    "        for hsp in hit.findall(\".//Hsp\"):\n",
    "            hseq_elem = hsp.find(\"Hsp_hseq\")\n",
    "            identity_elem = hsp.find(\"Hsp_identity\")\n",
    "            align_len_elem = hsp.find(\"Hsp_align-len\")\n",
    "            if hseq_elem is not None and identity_elem is not None and align_len_elem is not None:\n",
    "                hseq = hseq_elem.text.strip()\n",
    "                identity = int(identity_elem.text)\n",
    "                align_len = int(align_len_elem.text)\n",
    "                identity_pct = 100 * identity / align_len\n",
    "                if identity_pct < identity_threshold and len(hseq) > min_length:\n",
    "                    seqs.append(hseq)\n",
    "    seqs = [wt_sequence] + list({s for s in seqs if s != wt_sequence})  # unique, include WT\n",
    "    print(f\"Total homologs: {len(seqs)}\")\n",
    "    # Save to FASTA\n",
    "    with open(\"msa_input.fasta\", \"w\") as f:\n",
    "        for i, s in enumerate(seqs):\n",
    "            f.write(f\">seq{i}\\n{s}\\n\")\n",
    "    return \"msa_input.fasta\"\n",
    "\n",
    "# ======= Step 2: Align with MAFFT =======\n",
    "def run_mafft(input_fasta, output_fasta=\"msa_aligned.fasta\"):\n",
    "    print(f\"Running MAFFT alignment...\")\n",
    "    cmd = f\"mafft --auto {input_fasta} > {output_fasta}\"\n",
    "    subprocess.run(cmd, shell=True, check=True)\n",
    "    print(f\"Alignment written: {output_fasta}\")\n",
    "    return output_fasta\n",
    "\n",
    "# ======= Step 3: Calculate Henikoff Weights =======\n",
    "def henikoff_weights(msa_file, format=\"fasta\"):\n",
    "    alignment = AlignIO.read(msa_file, format)\n",
    "    n_seq = len(alignment)\n",
    "    aln_len = alignment.get_alignment_length()\n",
    "    weights = np.zeros(n_seq)\n",
    "    for pos in range(aln_len):\n",
    "        aa_counts = {}\n",
    "        for record in alignment:\n",
    "            aa = record.seq[pos]\n",
    "            if aa not in aa_counts:\n",
    "                aa_counts[aa] = 0\n",
    "            aa_counts[aa] += 1\n",
    "        n_types = len(aa_counts)\n",
    "        for i, record in enumerate(alignment):\n",
    "            aa = record.seq[pos]\n",
    "            weights[i] += 1.0 / (n_types * aa_counts[aa])\n",
    "    weights /= weights.sum()\n",
    "    return weights\n",
    "\n",
    "# ======= (Optional) Jackhmmer/MMseqs2 integration (not changed here) =======\n",
    "\n",
    "# ==== MAIN WORKFLOW ====\n",
    "method = \"blast\"  # \"jackhmmer\" or \"mmseqs2\" possible if implemented\n",
    "\n",
    "if method == \"blast\":\n",
    "    msa_input = run_blast_search(WT_SEQUENCE, identity_threshold=90.0, hitlist_size=500)\n",
    "elif method == \"jackhmmer\":\n",
    "    msa_input = run_jackhmmer_search(WT_SEQUENCE)\n",
    "elif method == \"mmseqs2\":\n",
    "    msa_input = run_mmseqs2_search(WT_SEQUENCE)\n",
    "else:\n",
    "    raise ValueError(\"Invalid method chosen. Please select 'blast', 'jackhmmer', or 'mmseqs2'.\")\n",
    "\n",
    "msa_aligned = run_mafft(msa_input)\n",
    "\n",
    "weights = henikoff_weights(msa_aligned, \"fasta\")\n",
    "print(\"Sequence weights:\", weights)\n",
    "np.save(\"msa_weights.npy\", weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-09T15:55:23.485746Z",
     "iopub.status.busy": "2025-06-09T15:55:23.485410Z",
     "iopub.status.idle": "2025-06-09T16:00:48.336546Z",
     "shell.execute_reply": "2025-06-09T16:00:48.335856Z",
     "shell.execute_reply.started": "2025-06-09T15:55:23.485724Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch.optim import AdamW\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import os\n",
    "\n",
    "# --- MEMORY OPTIMIZATION SETTINGS ---\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True,max_split_size_mb:128\"\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "def force_cleanup():\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "force_cleanup()\n",
    "\n",
    "# --- DEVICE ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_props = torch.cuda.get_device_properties(0)\n",
    "    print(f\"GPU: {gpu_props.name}\")\n",
    "    print(f\"Total memory: {gpu_props.total_memory/1024**3:.2f} GB\")\n",
    "\n",
    "# --- Load and preprocess data ---\n",
    "url = \"https://figshare.com/ndownloader/files/7337543\"\n",
    "df = pd.read_csv(url, sep='\\t')\n",
    "df.rename(columns={'mutation': 'mutation_string', 'normalized_fitness': 'fitness'}, inplace=True)\n",
    "df['fitness'] = pd.to_numeric(df['fitness'], errors='coerce')\n",
    "df.dropna(subset=['fitness'], inplace=True)\n",
    "\n",
    "# Use moderate sample size for stable training\n",
    "df = df.sample(n=1000, random_state=42).reset_index(drop=True)\n",
    "print(f\"Using {len(df)} samples for training\")\n",
    "\n",
    "# Check fitness distribution and normalize\n",
    "print(f\"Fitness range: {df['fitness'].min():.4f} to {df['fitness'].max():.4f}\")\n",
    "print(f\"Fitness mean: {df['fitness'].mean():.4f}, std: {df['fitness'].std():.4f}\")\n",
    "\n",
    "# CRITICAL FIX: Normalize fitness values to prevent extreme values\n",
    "fitness_mean = df['fitness'].mean()\n",
    "fitness_std = df['fitness'].std()\n",
    "df['fitness_normalized'] = (df['fitness'] - fitness_mean) / fitness_std\n",
    "print(f\"Normalized fitness range: {df['fitness_normalized'].min():.4f} to {df['fitness_normalized'].max():.4f}\")\n",
    "\n",
    "WT_SEQUENCE = \"MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKFICTTGKLPVPWPTLVTTLTYGVQCFSRYPDHMKQHDFFKSAMPEGYVQERTIFFKDDGNYKTRAEVKFEGDTLVNRIELKGIDFKEDGNILGHKLEYNYNSHNVYIMADKQKNGIKVNFKIRHNIEDGSVQLADHYQQNTPIGDGPVLLPDNHYLSTQSALSKDPNEKRDHMVLLEFVTAAGITHGMDELYK\"\n",
    "\n",
    "def generate_mutated_sequence(wt_sequence, mutation_string):\n",
    "    seq_list = list(wt_sequence)\n",
    "    mutations = mutation_string.split(',')\n",
    "    for mut in mutations:\n",
    "        mut = mut.strip()\n",
    "        if len(mut) >= 3 and mut[1:-1].isdigit():\n",
    "            pos = int(mut[1:-1]) - 1\n",
    "            if 0 <= pos < len(seq_list) and mut[-1] != '*':\n",
    "                seq_list[pos] = mut[-1]\n",
    "    return ''.join(seq_list)\n",
    "\n",
    "df['mutated_sequence'] = df['mutation_string'].apply(lambda x: generate_mutated_sequence(WT_SEQUENCE, x))\n",
    "\n",
    "# CRITICAL FIX: Proper sequence formatting for ProtBERT\n",
    "def format_protein_sequence(sequence):\n",
    "    \"\"\"Format protein sequence with spaces between amino acids as expected by ProtBERT\"\"\"\n",
    "    return ' '.join(list(sequence))\n",
    "\n",
    "df['formatted_sequence'] = df['mutated_sequence'].apply(format_protein_sequence)\n",
    "\n",
    "# --- Load tokenizer and model ---\n",
    "MODEL_NAME = \"Rostlab/prot_bert\"\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME, do_lower_case=False)\n",
    "\n",
    "force_cleanup()\n",
    "\n",
    "print(\"Loading model...\")\n",
    "base_model = BertModel.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float32,\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "# --- FIXED LoRA config based on research ---\n",
    "lora_config = LoraConfig(\n",
    "    r=8,  # Increased rank for better expressiveness\n",
    "    lora_alpha=32,  # Higher alpha for stronger adaptation\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"query\", \"key\", \"value\"],  # All attention components\n",
    "    bias=\"none\"\n",
    ")\n",
    "\n",
    "print(\"Applying LoRA...\")\n",
    "base_model = get_peft_model(base_model, lora_config)\n",
    "base_model = base_model.to(device)\n",
    "\n",
    "force_cleanup()\n",
    "\n",
    "# --- FIXED Dataset ---\n",
    "class ProteinFitnessDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, target_col, max_length=512):\n",
    "        self.sequences = dataframe['formatted_sequence'].tolist()  # Use formatted sequences\n",
    "        self.targets = dataframe[target_col].values.astype(np.float32)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Debug: Print first few sequences\n",
    "        print(f\"First sequence example: {self.sequences[0][:100]}...\")\n",
    "        print(f\"Target range: {self.targets.min():.4f} to {self.targets.max():.4f}\")\n",
    "    \n",
    "    def __len__(self): \n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.sequences[idx]\n",
    "        \n",
    "        # CRITICAL: Ensure sequence is properly tokenized\n",
    "        tokenized = self.tokenizer(\n",
    "            seq, \n",
    "            padding='max_length', \n",
    "            truncation=True, \n",
    "            max_length=self.max_length, \n",
    "            return_tensors=\"pt\",\n",
    "            add_special_tokens=True  # Ensure [CLS] and [SEP] tokens\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': tokenized['input_ids'].squeeze(0),\n",
    "            'attention_mask': tokenized['attention_mask'].squeeze(0),\n",
    "            'labels': torch.tensor(self.targets[idx], dtype=torch.float32)\n",
    "        }\n",
    "\n",
    "# Split data with stratification to ensure balanced distribution\n",
    "train_df, test_df = train_test_split(df, test_size=0.3, random_state=42, \n",
    "                                   stratify=pd.cut(df['fitness_normalized'], bins=5, labels=False))\n",
    "print(f\"Train size: {len(train_df)}, Test size: {len(test_df)}\")\n",
    "\n",
    "# Create datasets using normalized fitness\n",
    "train_dataset = ProteinFitnessDataset(train_df, tokenizer, 'fitness_normalized')\n",
    "test_dataset = ProteinFitnessDataset(test_df, tokenizer, 'fitness_normalized')\n",
    "\n",
    "# Use batch size of 4 for better gradient estimates\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, num_workers=0)\n",
    "\n",
    "# --- FIXED Regression Head ---\n",
    "class ProtBERTRegressionHead(torch.nn.Module):\n",
    "    def __init__(self, encoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        \n",
    "        # CRITICAL FIX: Better architecture for regression\n",
    "        hidden_size = self.encoder.config.hidden_size\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "        \n",
    "        self.regressor = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_size, 512),\n",
    "            torch.nn.LayerNorm(512),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.2),\n",
    "            torch.nn.Linear(512, 128),\n",
    "            torch.nn.LayerNorm(128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.2),\n",
    "            torch.nn.Linear(128, 32),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(32, 1)\n",
    "        )\n",
    "        \n",
    "        # CRITICAL: Proper initialization\n",
    "        for module in self.regressor:\n",
    "            if isinstance(module, torch.nn.Linear):\n",
    "                torch.nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if module.bias is not None:\n",
    "                    torch.nn.init.zeros_(module.bias)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        # Get encoder outputs\n",
    "        with torch.cuda.amp.autocast():  # Use mixed precision\n",
    "            encoder_outputs = self.encoder(\n",
    "                input_ids=input_ids, \n",
    "                attention_mask=attention_mask,\n",
    "                output_hidden_states=True\n",
    "            )\n",
    "        \n",
    "        # CRITICAL FIX: Better pooling strategy\n",
    "        hidden_states = encoder_outputs.last_hidden_state\n",
    "        \n",
    "        # Use mean pooling over sequence length, weighted by attention mask\n",
    "        attention_mask_expanded = attention_mask.unsqueeze(-1).expand(hidden_states.size()).float()\n",
    "        sum_embeddings = torch.sum(hidden_states * attention_mask_expanded, 1)\n",
    "        sum_mask = torch.clamp(attention_mask_expanded.sum(1), min=1e-9)\n",
    "        pooled_output = sum_embeddings / sum_mask\n",
    "        \n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        predictions = self.regressor(pooled_output).squeeze(-1)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Use smooth L1 loss for more stable training\n",
    "            loss_fn = torch.nn.SmoothL1Loss(beta=1.0)\n",
    "            loss = loss_fn(predictions, labels)\n",
    "        \n",
    "        return predictions, loss\n",
    "\n",
    "print(\"Creating regression model...\")\n",
    "regression_model = ProtBERTRegressionHead(base_model).to(device)\n",
    "\n",
    "# CRITICAL FIX: Much smaller learning rate for stability\n",
    "optimizer = AdamW(regression_model.parameters(), lr=1e-5, weight_decay=0.01, eps=1e-8)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=1e-7)\n",
    "\n",
    "# Use GradScaler for mixed precision training\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "epochs = 10  # More epochs for better convergence\n",
    "\n",
    "def train_epoch(model, loader, optimizer, scheduler, scaler):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for batch in tqdm(loader, desc=\"Training\"):\n",
    "        try:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            \n",
    "            # Forward pass with mixed precision\n",
    "            with torch.cuda.amp.autocast():\n",
    "                predictions, loss = model(input_ids, attention_mask, labels)\n",
    "            \n",
    "            # Check for NaN loss\n",
    "            if torch.isnan(loss) or torch.isinf(loss):\n",
    "                print(f\"NaN/Inf loss detected: {loss.item()}\")\n",
    "                continue\n",
    "            \n",
    "            # Backward pass with gradient scaling\n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            \n",
    "        except RuntimeError as e:\n",
    "            print(f\"Error in training batch: {e}\")\n",
    "            force_cleanup()\n",
    "            continue\n",
    "    \n",
    "    scheduler.step()\n",
    "    return total_loss / max(num_batches, 1)\n",
    "\n",
    "def eval_epoch(model, loader):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Evaluating\"):\n",
    "            try:\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                labels = batch[\"labels\"]\n",
    "                \n",
    "                with torch.cuda.amp.autocast():\n",
    "                    predictions, _ = model(input_ids, attention_mask)\n",
    "                \n",
    "                y_true.extend(labels.cpu().numpy().tolist())\n",
    "                y_pred.extend(predictions.cpu().numpy().tolist())\n",
    "                \n",
    "            except RuntimeError as e:\n",
    "                print(f\"Error in evaluation batch: {e}\")\n",
    "                continue\n",
    "    \n",
    "    if len(y_true) < 2:\n",
    "        return 0, 0, 0, 0, 0, y_true, y_pred\n",
    "    \n",
    "    # Convert to numpy arrays and clean data\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    \n",
    "    # Remove NaN/Inf values\n",
    "    valid_mask = ~(np.isnan(y_true) | np.isnan(y_pred) | np.isinf(y_true) | np.isinf(y_pred))\n",
    "    y_true = y_true[valid_mask]\n",
    "    y_pred = y_pred[valid_mask]\n",
    "    \n",
    "    if len(y_true) < 2:\n",
    "        return 0, 0, 0, 0, 0, y_true.tolist(), y_pred.tolist()\n",
    "    \n",
    "    try:\n",
    "        mse = mean_squared_error(y_true, y_pred)\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "        \n",
    "        # Both Pearson and Spearman correlations\n",
    "        if np.std(y_pred) > 1e-10 and np.std(y_true) > 1e-10:\n",
    "            pearson = pearsonr(y_true, y_pred)[0]\n",
    "            spearman = spearmanr(y_true, y_pred)[0]\n",
    "        else:\n",
    "            pearson = spearman = 0.0\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating metrics: {e}\")\n",
    "        mse = mae = r2 = pearson = spearman = 0.0\n",
    "    \n",
    "    return mse, mae, r2, pearson, spearman, y_true.tolist(), y_pred.tolist()\n",
    "\n",
    "# --- Training loop with better monitoring ---\n",
    "print(\"Starting training...\")\n",
    "best_pearson = -1.0\n",
    "patience = 3\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "    \n",
    "    train_loss = train_epoch(regression_model, train_loader, optimizer, scheduler, scaler)\n",
    "    \n",
    "    if np.isnan(train_loss) or np.isinf(train_loss):\n",
    "        print(\"Training loss is NaN/Inf. Stopping training.\")\n",
    "        break\n",
    "    \n",
    "    mse, mae, r2, pearson, spearman, y_true, y_pred = eval_epoch(regression_model, test_loader)\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"Test MSE: {mse:.4f}, MAE: {mae:.4f}, R²: {r2:.4f}\")\n",
    "    print(f\"Pearson: {pearson:.4f}, Spearman: {spearman:.4f}\")\n",
    "    print(f\"Learning Rate: {scheduler.get_last_lr()[0]:.2e}\")\n",
    "    \n",
    "    # Early stopping based on Pearson correlation\n",
    "    if pearson > best_pearson:\n",
    "        best_pearson = pearson\n",
    "        patience_counter = 0\n",
    "        print(f\"New best Pearson correlation: {best_pearson:.4f}\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping: no improvement for {patience} epochs\")\n",
    "            break\n",
    "    \n",
    "    force_cleanup()\n",
    "\n",
    "# --- Final evaluation and plotting ---\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FINAL EVALUATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "if 'y_true' in locals() and len(y_true) > 1:\n",
    "    # Convert back to original scale for interpretation\n",
    "    y_true_orig = np.array(y_true) * fitness_std + fitness_mean\n",
    "    y_pred_orig = np.array(y_pred) * fitness_std + fitness_mean\n",
    "    \n",
    "    # Calculate metrics on original scale\n",
    "    mse_orig = mean_squared_error(y_true_orig, y_pred_orig)\n",
    "    mae_orig = mean_absolute_error(y_true_orig, y_pred_orig)\n",
    "    r2_orig = r2_score(y_true_orig, y_pred_orig)\n",
    "    pearson_orig = pearsonr(y_true_orig, y_pred_orig)[0] if len(y_true_orig) > 1 else 0\n",
    "    \n",
    "    print(f\"Final Results (Original Scale):\")\n",
    "    print(f\"  Test samples: {len(y_true_orig)}\")\n",
    "    print(f\"  MSE: {mse_orig:.4f}\")\n",
    "    print(f\"  MAE: {mae_orig:.4f}\")\n",
    "    print(f\"  R²: {r2_orig:.4f}\")\n",
    "    print(f\"  Pearson: {pearson_orig:.4f}\")\n",
    "    \n",
    "    # Plot results\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Plot on original scale\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.scatter(y_true_orig, y_pred_orig, alpha=0.6, s=30)\n",
    "    min_val = min(min(y_true_orig), min(y_pred_orig))\n",
    "    max_val = max(max(y_true_orig), max(y_pred_orig))\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.8)\n",
    "    plt.xlabel(\"True Fitness\")\n",
    "    plt.ylabel(\"Predicted Fitness\")\n",
    "    plt.title(f\"Original Scale (R²={r2_orig:.3f}, ρ={pearson_orig:.3f})\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot on normalized scale\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.scatter(y_true, y_pred, alpha=0.6, s=30)\n",
    "    min_val = min(min(y_true), min(y_pred))\n",
    "    max_val = max(max(y_true), max(y_pred))\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.8)\n",
    "    plt.xlabel(\"True Fitness (Normalized)\")\n",
    "    plt.ylabel(\"Predicted Fitness (Normalized)\")\n",
    "    plt.title(f\"Normalized Scale (R²={r2:.3f}, ρ={pearson:.3f})\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Residual plot\n",
    "    plt.subplot(2, 2, 3)\n",
    "    residuals = np.array(y_pred_orig) - np.array(y_true_orig)\n",
    "    plt.scatter(y_true_orig, residuals, alpha=0.6, s=30)\n",
    "    plt.axhline(y=0, color='r', linestyle='--', alpha=0.8)\n",
    "    plt.xlabel(\"True Fitness\")\n",
    "    plt.ylabel(\"Residuals\")\n",
    "    plt.title(\"Residual Plot\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Distribution comparison\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.hist(y_true_orig, bins=20, alpha=0.5, label='True', density=True)\n",
    "    plt.hist(y_pred_orig, bins=20, alpha=0.5, label='Predicted', density=True)\n",
    "    plt.xlabel(\"Fitness\")\n",
    "    plt.ylabel(\"Density\")\n",
    "    plt.title(\"Distribution Comparison\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Show example predictions\n",
    "    print(f\"\\nExample predictions (original scale):\")\n",
    "    indices = np.random.choice(len(y_true_orig), min(10, len(y_true_orig)), replace=False)\n",
    "    for i in indices:\n",
    "        print(f\"  True: {y_true_orig[i]:.4f}, Pred: {y_pred_orig[i]:.4f}, Error: {abs(y_true_orig[i] - y_pred_orig[i]):.4f}\")\n",
    "else:\n",
    "    print(\"No valid results to display.\")\n",
    "\n",
    "force_cleanup()\n",
    "print(f\"Final GPU memory usage: {torch.cuda.memory_allocated()/1024**3:.2f} GB\" if torch.cuda.is_available() else \"CPU mode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
