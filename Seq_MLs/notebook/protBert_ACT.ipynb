{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 137176,
     "status": "ok",
     "timestamp": 1742650645683,
     "user": {
      "displayName": "Ryoji Takahashi",
      "userId": "08099237406056068712"
     },
     "user_tz": -60
    },
    "id": "7TfzYEa2qLz4",
    "outputId": "c0fb4012-1f22-4dff-a19b-3403d6192704"
   },
   "outputs": [],
   "source": [
    "# Install necessary packages\n",
    "!pip install torch transformers scikit-learn pandas numpy biopython peft bitsandbytes requests optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23871,
     "status": "ok",
     "timestamp": 1742650669570,
     "user": {
      "displayName": "Ryoji Takahashi",
      "userId": "08099237406056068712"
     },
     "user_tz": -60
    },
    "id": "oCl5Gk3GUaM_",
    "outputId": "ce97b5e3-f603-4c2b-a716-6b306a6c4141"
   },
   "outputs": [],
   "source": [
    "# Install necessary tools in Google Colab\n",
    "!apt-get install -y hmmer\n",
    "!wget https://github.com/soedinglab/MMseqs2/releases/download/17-b804f/mmseqs-linux-gpu.tar.gz\n",
    "!tar xvf mmseqs-linux-gpu.tar.gz\n",
    "!chmod +x mmseqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "0c612b3640044c7ebd71addcde9ced72",
      "da307c320af342e98b825a96a197a7dc",
      "7b19f04e6bfe410e8a0a317a251bb9a8",
      "71747f0b96ec4a27ab16e61d12b53619",
      "b4932c25503d44e7b230ba212261c230",
      "12b756dcf8fb4a7eb7b44866348ec27e",
      "b3fd28b80fa04b0c9a0ac11321c150ea",
      "c313acb95ed24aeb90f0b9b501b2eb0e",
      "21b82ac01198443aa47bf252d37b60c6",
      "b63daad3c9024ec3941a1a444670c070",
      "50a31ad977ba4e308d185d2fa89de112",
      "50ab94e403b64f92bd2ce70fe623caad",
      "778123140f134a87b8f39d84dd22b2a3",
      "7ea73ba579c54212bed88775b0025dc6",
      "af35b6f69ffd4a1282f749832264341a",
      "b79c5bce60a14310aed52c34bdcac665",
      "e2436ade453040f38092b506d3d643bc",
      "ec51b687539e4819919d47d5de287816",
      "d918413ff9a44e59be58098e628caeec",
      "f0a4e5bd40904edaad31223a256312dc",
      "a8461c0929c64aeb9aa9917875e1bd50",
      "0dc20ad7eff14ef980d5f7d2acb308fc",
      "b573f271ac844206a908456f14ffa656",
      "cf5d1f668166418bb7d04a3bd983c380",
      "34139b6d953941c296e00909c6550585",
      "4637a39c72e14cacad3c748479005287",
      "a9a98c30e1914ee58c11cb6a7f43a461",
      "b5aba31c7ae242fe887f3ebc209ed61c",
      "d9b1e7526298404a9aeb3ff2f43edd7e",
      "d5d36048c2a948bb9d0a6f1a66abe21d",
      "bb20afce1030419985b5b559b52fce90",
      "f7f40c96d28a489e9de26c5ac4b9fd0c",
      "f5236a66cca54eb296a59c22c9059598",
      "db40349d8c254cf2b0774f779c1cf7e9",
      "9cb65ef731d34f3d92c125e1cb81d8f7",
      "02b9125c916d46edaec385c770f50b7a",
      "fd03f82c8caa4ac398edaca0c437fe50",
      "b9cd6283056144578306cf0fda4c8ff8",
      "123b1d2f682d489995598aa6004a76cf",
      "bed93adcb3cd46238730ff1ae5b94d5d",
      "5988eedb571545c3844773966bc262fe",
      "1912299bda314408b07fe15a22a02b84",
      "c8cd6a8c41244adaa96548de5fa32cbd",
      "fcd0692ed55b4d8fbf2ed181e1193f5c",
      "3944573a47654ec69fb589c7fd1f11ca",
      "30028e1c602d4934a44ebb654bee2196",
      "69e8801ec44645648cf35025f43b719d",
      "e9950188d0c44977954dd185bc718517",
      "15a7f2f1f09b4adf9c49531c5fcd9eeb",
      "a057799156de426a9d30bd977505eb02",
      "6c93bd29dd3d4a9aa63cebe525ca1d37",
      "f77e5dbd8dfb4e389dec8654572fbcb6",
      "81892f95210f48559de697db590e044e",
      "b127d89066e64c048596f293b672687c",
      "d24e651a6f874a388a99e4d89ab0c56f",
      "eb05579a285a45cb9e2b5978092c71ba",
      "e60cb77ca6db4bee81cc94c340f3fef2",
      "fc7ebb24306c4488ac3b90a7dbb249b4",
      "6a70b6fc1acd4a15a56f0af196f2cbb4",
      "d633351cd08147a1b16bca7a87d09d5d",
      "56fd2cd9d39b4d94ac77faa10939c932",
      "94ab8f73ba324a7abcd14f51796633fa",
      "d00de2a066d846099af65e88966e46e3",
      "989a05586f4e4bd19494ef9fe5e48ba0",
      "7dba158b31664804a6335de65cb89730",
      "e591032694314fab9701bd8359d0ac91"
     ]
    },
    "executionInfo": {
     "elapsed": 612416,
     "status": "ok",
     "timestamp": 1742651281990,
     "user": {
      "displayName": "Ryoji Takahashi",
      "userId": "08099237406056068712"
     },
     "user_tz": -60
    },
    "id": "xGLKt38Kta0W",
    "outputId": "3fc9e563-e0bd-4212-e522-5e6545676897"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import requests\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, Trainer, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "from Bio import SeqIO\n",
    "import optuna\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import time\n",
    "import xml.etree.ElementTree as ET\n",
    "from scipy.stats import pearsonr\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "\n",
    "\n",
    "# Check for GPU availability (Colab T4 GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# URLs for external tools\n",
    "BLAST_URL = \"https://blast.ncbi.nlm.nih.gov/Blast.cgi\"\n",
    "\n",
    "# Wild-type enzyme sequence\n",
    "WT_SEQUENCE = \"MRHGDISSSNDTVGVAVVNYKMPRLHTAAEVLDNARKIAEMIVGMKQGLPGMDLVVFPEYSLQGIMYDPAEMMETAVAIPGEETEIFSRACRKANVWGVFSLTGERHEEHPRKAPYNTLVLIDNNGEIVQKYRKIIPWCPIEGWYPGGQTYVSEGPKGMKISLIICDDGNYPEIWRDCAMKGAELIVRCQGYMYPAKDQQVMMAKAMAWANNCYVAVANAAGFDGVYSYFGHSAIIGFDGRTLGECGEEEMGIQYAQLSLSQIRDARANDQSQNHLFKILHRGYSGLQASGDGDRGLAECPFEFYRTWVTDAEKARENVERLTRSTTGVAQCPVGRLPYEG\"\n",
    "\n",
    "\n",
    "def run_blast_search(wt_sequence, identity_threshold=98.0, max_retries=50, sleep_time=20):\n",
    "    \"\"\"Run BLAST search and extract homologous sequences below a given identity threshold.\"\"\"\n",
    "\n",
    "    # **1. Submit BLAST Query**\n",
    "    params = {\n",
    "        \"CMD\": \"Put\",\n",
    "        \"PROGRAM\": \"blastp\",\n",
    "        \"DATABASE\": \"nr\",\n",
    "        \"QUERY\": wt_sequence,\n",
    "        \"FORMAT_TYPE\": \"XML\",\n",
    "        \"EXPECT\": \"10\",  # Set e-value threshold (adjustable)\n",
    "        \"HITLIST_SIZE\": \"500\"  # Increase number of results\n",
    "    }\n",
    "\n",
    "    response = requests.post(BLAST_URL, data=params)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Error submitting sequence: {response.text}\")\n",
    "\n",
    "    response_text = response.text\n",
    "    if \"RID = \" not in response_text:\n",
    "        raise Exception(\"Error: No RID found in response.\")\n",
    "\n",
    "    rid = response_text.split(\"RID = \")[-1].split(\"\\n\")[0].strip()\n",
    "    print(f\"ðŸ”µ Submitted BLAST job with RID: {rid}\")\n",
    "\n",
    "    # **2. Wait for BLAST results**\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "        status_params = {\"CMD\": \"Get\", \"FORMAT_OBJECT\": \"SearchInfo\", \"RID\": rid}\n",
    "        status_response = requests.get(BLAST_URL, params=status_params)\n",
    "\n",
    "        if \"Status=READY\" in status_response.text:\n",
    "            print(\"ðŸŸ¢ BLAST search is complete.\")\n",
    "            break\n",
    "        elif \"Status=FAILED\" in status_response.text:\n",
    "            raise Exception(\"ðŸ”´ BLAST job failed on NCBI server.\")\n",
    "\n",
    "        print(f\"â³ Waiting for BLAST results... (Attempt {retries + 1}/{max_retries})\")\n",
    "        time.sleep(sleep_time)\n",
    "        retries += 1\n",
    "\n",
    "    if retries == max_retries:\n",
    "        raise Exception(\"ðŸ”´ BLAST API timed out. Try increasing max_retries or sleep_time.\")\n",
    "\n",
    "    # **3. Retrieve BLAST results**\n",
    "    result_params = {\"CMD\": \"Get\", \"FORMAT_TYPE\": \"XML\", \"RID\": rid}\n",
    "    result_response = requests.get(BLAST_URL, params=result_params)\n",
    "    if result_response.status_code != 200:\n",
    "        raise Exception(f\"Error retrieving results: {result_response.text}\")\n",
    "\n",
    "    # Print raw response for debugging\n",
    "    print(result_response.text[:1000])  # Print first 1000 characters to check if XML is valid\n",
    "\n",
    "    # **4. Parse BLAST results**\n",
    "    root = ET.fromstring(result_response.text)\n",
    "    homologous_sequences = []\n",
    "    query_sequences = []\n",
    "    identities = []\n",
    "\n",
    "    for hit in root.findall(\".//Hit\"):\n",
    "        hit_id = hit.find(\"Hit_id\").text if hit.find(\"Hit_id\") is not None else \"Unknown\"\n",
    "        hit_def = hit.find(\"Hit_def\").text if hit.find(\"Hit_def\") is not None else \"Unknown\"\n",
    "\n",
    "        for hsp in hit.findall(\".//Hsp\"):\n",
    "            hsp_qseq_elem = hsp.find(\"Hsp_qseq\")\n",
    "            hsp_hseq_elem = hsp.find(\"Hsp_hseq\")\n",
    "            identity_elem = hsp.find(\"Hsp_identity\")\n",
    "            align_len_elem = hsp.find(\"Hsp_align-len\")\n",
    "\n",
    "            if hsp_qseq_elem is None or hsp_hseq_elem is None or identity_elem is None or align_len_elem is None:\n",
    "                continue  # Skip if missing data\n",
    "\n",
    "            qseq = hsp_qseq_elem.text.strip()\n",
    "            hseq = hsp_hseq_elem.text.strip()\n",
    "            identity = int(identity_elem.text)\n",
    "            align_len = int(align_len_elem.text)\n",
    "            identity_percentage = (identity / align_len) * 100\n",
    "\n",
    "            if identity_percentage <= identity_threshold:\n",
    "                homologous_sequences.append(hseq)  # Homologous sequences\n",
    "                query_sequences.append(qseq)  # Query sequences\n",
    "                identities.append(identity_percentage)\n",
    "                print(f\"âœ… Found sequence from {hit_id}: {hit_def} (Identity: {identity_percentage:.2f}%)\")\n",
    "\n",
    "    print(f\"âœ… Total homologous sequences collected: {len(homologous_sequences)}\")\n",
    "    print(f\"âœ… Total query sequences collected: {len(query_sequences)}\")\n",
    "\n",
    "    return homologous_sequences, query_sequences, identities\n",
    "\n",
    "# **Run BLAST API Search**\n",
    "homologous_sequences, _, _ = run_blast_search(WT_SEQUENCE)\n",
    "\n",
    "\n",
    "print(f\"ðŸ”¹ Retrieved {len(homologous_sequences)} homologous sequences.\")\n",
    "#print(f\"ðŸ”¹ Retrieved {len(query_sequences)} query sequences.\")\n",
    "#print(f\"ðŸ”¹ Average Sequence Identity: {sum(identities)/len(identities) if identities else 0:.2f}%\")\n",
    "\n",
    "\n",
    "def run_jackhmmer_search(wt_sequence, num_iter=3, evalue=1e-5):\n",
    "    \"\"\"Run Jackhmmer search using HMMER locally. Assumes HMMER is installed.\"\"\"\n",
    "    fasta_filename = \"wt_sequence.fasta\"\n",
    "    with open(fasta_filename, \"w\") as f:\n",
    "        f.write(f\">WT_SEQUENCE\\n{wt_sequence}\\n\")\n",
    "\n",
    "    output_file = \"jackhmmer_output.txt\"\n",
    "    cmd = f\"jackhmmer --tblout {output_file} -N {num_iter} --cpu 4 -E {evalue} {fasta_filename} /usr/share/uniprot_sprot.fasta\"\n",
    "\n",
    "    subprocess.run(cmd, shell=True, check=True)\n",
    "\n",
    "    sequences = []\n",
    "    with open(output_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            if not line.startswith(\"#\"):\n",
    "                columns = line.strip().split()\n",
    "                if len(columns) > 1:\n",
    "                    sequences.append(columns[0])  # Extract sequence ID\n",
    "\n",
    "    print(f\"Total sequences collected from Jackhmmer: {len(sequences)}\")\n",
    "    return sequences if sequences else [wt_sequence]\n",
    "\n",
    "\n",
    "def run_mmseqs2_search(wt_sequence):\n",
    "    \"\"\"Run MMseqs2 locally in Google Colab.\"\"\"\n",
    "\n",
    "    # Save WT sequence to FASTA file\n",
    "    fasta_filename = \"wt_sequence.fasta\"\n",
    "    with open(fasta_filename, \"w\") as f:\n",
    "        f.write(f\">WT_SEQUENCE\\n{wt_sequence}\\n\")\n",
    "\n",
    "    db_name = \"uniref50\"\n",
    "    output_file = \"mmseqs_output.m8\"\n",
    "\n",
    "    # Create database, search, and convert output\n",
    "    subprocess.run(f\"mmseqs createdb {fasta_filename} wt_sequence_db\", shell=True, check=True)\n",
    "    subprocess.run(f\"mmseqs search wt_sequence_db {db_name} results tmp --threads 4\", shell=True, check=True)\n",
    "    subprocess.run(f\"mmseqs convertalis wt_sequence_db {db_name} results {output_file}\", shell=True, check=True)\n",
    "\n",
    "    sequences = []\n",
    "    with open(output_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            columns = line.strip().split(\"\\t\")\n",
    "            if len(columns) > 1:\n",
    "                sequences.append(columns[1])  # Extract sequence ID\n",
    "\n",
    "    print(f\"Total sequences collected from MMseqs2: {len(sequences)}\")\n",
    "    return sequences if sequences else [wt_sequence]\n",
    "\n",
    "\n",
    "# Example Usage\n",
    "method = \"blast\"  # Change to \"jackhmmer\" or \"mmseqs2\" as needed\n",
    "\n",
    "if method == \"blast\":\n",
    "    homologous_sequences, _, _ = run_blast_search(WT_SEQUENCE)\n",
    "elif method == \"jackhmmer\":\n",
    "    homologous_sequences = run_jackhmmer_search(WT_SEQUENCE)\n",
    "elif method == \"mmseqs2\":\n",
    "    homologous_sequences = run_mmseqs2_search(WT_SEQUENCE)\n",
    "else:\n",
    "    raise ValueError(\"Invalid method chosen. Please select 'blast', 'jackhmmer', or 'mmseqs2'.\")\n",
    "\n",
    "print(f\"Retrieved {len(homologous_sequences)} sequences using {method}.\")\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load ProtBERT Tokenizer and Model\n",
    "print(\"Loading tokenizer and model...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Rostlab/prot_bert\")\n",
    "\n",
    "# Avoid float16 for CPU memory safety\n",
    "base_model = AutoModelForMaskedLM.from_pretrained(\"Rostlab/prot_bert\").to(device)\n",
    "\n",
    "# Embedding extractor\n",
    "\n",
    "def get_protbert_embedding(model, sequence):\n",
    "    if not isinstance(sequence, str) or len(sequence) == 0:\n",
    "        raise ValueError(\"Invalid input: `sequence` must be a non-empty string.\")\n",
    "    with torch.no_grad():\n",
    "        tokens = tokenizer(sequence, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "        outputs = model(**tokens, output_hidden_states=True)\n",
    "        if outputs.hidden_states:\n",
    "            last_hidden_state = outputs.hidden_states[-1]\n",
    "            embedding = last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
    "            return embedding\n",
    "        else:\n",
    "            raise AttributeError(\"Model outputs do not contain 'hidden_states'.\")\n",
    "\n",
    "# Mutation utility\n",
    "\n",
    "def generate_mutated_sequence(wt_sequence, mutation_string):\n",
    "    seq_list = list(wt_sequence)\n",
    "    mutations = mutation_string.split(',')\n",
    "    for mut in mutations:\n",
    "        if len(mut) >= 3 and mut[1:-1].isdigit():\n",
    "            pos = int(mut[1:-1]) - 1\n",
    "            if pos < len(seq_list) and mut[-1] != '*':\n",
    "                seq_list[pos] = mut[-1]\n",
    "    return ''.join(seq_list)\n",
    "\n",
    "# Load data\n",
    "print(\"Loading dataset...\")\n",
    "url = \"https://figshare.com/ndownloader/files/7337543\"\n",
    "df = pd.read_csv(url, sep='\\t')\n",
    "df.rename(columns={'mutation': 'mutation_string', 'normalized_fitness': 'fitness'}, inplace=True)\n",
    "df['fitness'] = pd.to_numeric(df['fitness'], errors='coerce')\n",
    "df.dropna(subset=['fitness'], inplace=True)\n",
    "df = df.sample(20, random_state=1)  # Reduce for memory safety\n",
    "\n",
    "# Wildtype\n",
    "WT_SEQUENCE = \"MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKFICTTGKLPVPWPTLVTTLTYGVQCFSRYPDHMKQHDFFKSAMPEGYVQERTIFFKDDGNYKTRAEVKFEGDTLVNRIELKGIDFKEDGNILGHKLEYNYNSHNVYIMADKQKNGIKVNFKIRHNIEDGSVQLADHYQQNTPIGDGPVLLPDNHYLSTQSALSKDPNEKRDHMVLLEFVTAAGITHGMDELYK\"\n",
    "df['mutated_sequence'] = df['mutation_string'].apply(lambda x: generate_mutated_sequence(WT_SEQUENCE, x))\n",
    "\n",
    "# Generate embeddings\n",
    "print(\"Generating embeddings...\")\n",
    "embeddings = []\n",
    "y = []\n",
    "for i, row in df.iterrows():\n",
    "    try:\n",
    "        emb = get_protbert_embedding(base_model, row['mutated_sequence'])\n",
    "        embeddings.append(emb)\n",
    "        y.append(row['fitness'])\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping sequence due to error: {e}\")\n",
    "\n",
    "X = np.vstack(embeddings)\n",
    "y = np.array(y)\n",
    "\n",
    "# Scale\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# ðŸ‘‡ This is key: get the indices first\n",
    "all_indices = np.arange(len(X_scaled))\n",
    "X_train_index, X_test_index = train_test_split(all_indices, test_size=0.2, random_state=42)\n",
    "\n",
    "# Get the actual data from the indices\n",
    "X_train = X_scaled[X_train_index]\n",
    "X_test = X_scaled[X_test_index]\n",
    "y_train = y[X_train_index]\n",
    "y_test = y[X_test_index]\n",
    "\n",
    "# Dummy train_dataloader for this example (no LoRA training actually done here)\n",
    "train_dataloader = None  # Replace with real one in full setup\n",
    "\n",
    "# Objective function for Optuna\n",
    "def objective(trial, model_name, dataloader, X_test, y_test):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = AutoModelForMaskedLM.from_pretrained(model_name).to(device)\n",
    "\n",
    "    r = trial.suggest_int(\"r\", 4, 64, step=4)\n",
    "    alpha = trial.suggest_int(\"alpha\", 8, 128, step=8)\n",
    "    lora_config = LoraConfig(\n",
    "        r=r,\n",
    "        lora_alpha=alpha,\n",
    "        target_modules=[\"query\", \"value\"],\n",
    "        lora_dropout=0.1\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(model, lora_config).to(device)\n",
    "\n",
    "    # Inference without training â€” use mean pooling to reduce embedding to scalar\n",
    "    y_pred = [np.mean(get_protbert_embedding(model, seq)) for seq in X_test]\n",
    "\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_test = np.array(y_test)\n",
    "    assert y_pred.shape == y_test.shape, f\"Shape mismatch: {y_pred.shape} vs {y_test.shape}\"\n",
    "\n",
    "    return mean_squared_error(y_test, y_pred)\n",
    "\n",
    "\n",
    "\n",
    "# Run Optuna (1 trial for safety)\n",
    "print(\"Running Optuna study...\")\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(lambda trial: objective(\n",
    "    trial,\n",
    "    \"Rostlab/prot_bert\",\n",
    "    train_dataloader,\n",
    "    df.iloc[X_test_index]['mutated_sequence'].tolist(),\n",
    "    df.iloc[X_test_index]['fitness'].values), n_trials=20)\n",
    "\n",
    "print(\"Best params:\", study.best_params)\n",
    "\n",
    "\n",
    "# Custom Dataset Class for Fine-tuning\n",
    "class ProteinDataset(Dataset):\n",
    "    def __init__(self, sequences, tokenizer, max_length=512):\n",
    "        self.sequences = sequences\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.sequences[idx]\n",
    "        inputs = self.tokenizer(sequence, truncation=True, padding='max_length', max_length=self.max_length, return_tensors=\"pt\")\n",
    "        return {\n",
    "            \"input_ids\": inputs[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": inputs[\"attention_mask\"].squeeze(0),\n",
    "        }\n",
    "\n",
    "# Fine-tune ProtBERT\n",
    "def train_protbert(model, dataloader):\n",
    "    if dataloader is None or not isinstance(dataloader, DataLoader):\n",
    "        print(\"Skipping training: No valid dataloader found for fine-tuning.\")\n",
    "        return\n",
    "\n",
    "    model.train()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-5, eps=1e-8)\n",
    "\n",
    "    for epoch in range(1):\n",
    "        for batch in dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "            input_ids = batch[\"input_ids\"]\n",
    "            attention_mask = batch[\"attention_mask\"]\n",
    "            labels = input_ids.clone()\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            if outputs.loss is None:\n",
    "                print(\"âš ï¸ Warning: Model returned no loss. Skipping batch.\")\n",
    "                continue\n",
    "\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        print(f\"âœ… Epoch {epoch+1} completed\")\n",
    "\n",
    "def objective(trial, model_name, dataloader, X_test, y_test):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = AutoModelForMaskedLM.from_pretrained(model_name).to(device)\n",
    "\n",
    "    r = trial.suggest_int(\"r\", 2, 16, step=2)\n",
    "    alpha = trial.suggest_int(\"alpha\", 4, 32, step=4)\n",
    "    lora_config = LoraConfig(\n",
    "        r=r,\n",
    "        lora_alpha=alpha,\n",
    "        target_modules=[\"query\", \"value\"],\n",
    "        lora_dropout=0.1\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(model, lora_config).to(device)\n",
    "\n",
    "    # Debug: print shape of a sample embedding\n",
    "    sample_emb = get_protbert_embedding(model, X_test[0])\n",
    "    print(\"Embedding shape (sample):\", np.array(sample_emb).shape)\n",
    "\n",
    "    # Convert embeddings to scalar using mean pooling\n",
    "    y_pred = [np.mean(np.array(get_protbert_embedding(model, seq)).squeeze()) for seq in X_test]\n",
    "\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_test = np.array(y_test)\n",
    "    assert y_pred.shape == y_test.shape, f\"Shape mismatch: {y_pred.shape} vs {y_test.shape}\"\n",
    "\n",
    "    return mean_squared_error(y_test, y_pred)\n",
    "\n",
    "\n",
    "\n",
    "# Prepare dataset and dataloader\n",
    "if not 'homologous_sequences' in globals() or not isinstance(homologous_sequences, list) or len(homologous_sequences) == 0:\n",
    "    raise ValueError(\"Error: No sequences available for training.\")\n",
    "\n",
    "train_dataset = ProteinDataset(homologous_sequences, tokenizer)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# Run Optuna Optimization\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(lambda trial: objective(\n",
    "    trial,\n",
    "    \"Rostlab/prot_bert\",\n",
    "    train_dataloader,\n",
    "    df.iloc[X_test_index]['mutated_sequence'].tolist(),\n",
    "    df.iloc[X_test_index]['fitness'].values), n_trials=20)\n",
    "\n",
    "\n",
    "# Apply best LoRA parameters\n",
    "best_lora_params = study.best_params\n",
    "print(f\"ðŸ† Best LoRA Parameters (Optuna): {best_lora_params}\")\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=best_lora_params[\"r\"],\n",
    "    lora_alpha=best_lora_params[\"alpha\"],\n",
    "    target_modules=[\"query\", \"value\"],\n",
    "    lora_dropout=0.1\n",
    ")\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"Rostlab/prot_bert\").to(device)\n",
    "model = get_peft_model(model, config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Final Fine-tuning\n",
    "train_protbert(model, train_dataloader)\n",
    "\n",
    "# Function to train, evaluate, and plot results\n",
    "def train_and_evaluate_model(model, param_grid, X_train, y_train, X_test, y_test, model_name):\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "    print(f\"{model_name} - Cross-validation MSE (mean): {-cv_scores.mean():.4f}\")\n",
    "\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    print(f\"Best parameters for {model_name}: {grid_search.best_params_}\")\n",
    "\n",
    "    y_pred = grid_search.predict(X_test)\n",
    "\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    print(f\"Mean Squared Error (MSE) - {model_name}: {mse:.4f}\")\n",
    "\n",
    "    pearson_corr, _ = pearsonr(y_test, y_pred)\n",
    "    print(f\"Pearson Correlation Coefficient - {model_name}: {pearson_corr:.4f}\")\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(y_test, y_pred, color='blue', alpha=0.6)\n",
    "    plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--')\n",
    "    plt.xlabel('Actual Fitness')\n",
    "    plt.ylabel('Predicted Fitness')\n",
    "    plt.title(f'Actual vs Predicted Fitness ({model_name})')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    return grid_search\n",
    "\n",
    "# Define parameter grids for each model\n",
    "param_grid_ridge = {'alpha': [0.01, 0.1, 1, 10, 100]}\n",
    "param_grid_lasso = {'alpha': [0.01, 0.1, 1, 10, 100]}\n",
    "param_grid_en = {'alpha': [0.01, 0.1, 1, 10, 100], 'l1_ratio': [0.1, 0.5, 0.9, 1.0]}\n",
    "\n",
    "# Define models\n",
    "ridge_model = Ridge()\n",
    "lasso_model = Lasso()\n",
    "elastic_net_model = ElasticNet()\n",
    "\n",
    "# Train and evaluate each model\n",
    "train_and_evaluate_model(ridge_model, param_grid_ridge, X_train, y_train, X_test, y_test, \"Ridge\")\n",
    "train_and_evaluate_model(lasso_model, param_grid_lasso, X_train, y_train, X_test, y_test, \"Lasso\")\n",
    "train_and_evaluate_model(elastic_net_model, param_grid_en, X_train, y_train, X_test, y_test, \"ElasticNet\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNhKinP9loPRXMd91St0csY",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
