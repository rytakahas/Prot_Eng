{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T07:21:21.869177Z",
     "iopub.status.busy": "2025-06-11T07:21:21.868935Z",
     "iopub.status.idle": "2025-06-11T07:23:07.542278Z",
     "shell.execute_reply": "2025-06-11T07:23:07.541598Z",
     "shell.execute_reply.started": "2025-06-11T07:21:21.869158Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ProtT5 LoRA Fine-tuning for Kaggle T4 GPU\n",
    "# Updated for Kaggle notebook environment\n",
    "\n",
    "# Install required packages (run this in a cell)\n",
    "!pip install transformers datasets evaluate deepspeed scipy scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T07:10:13.174521Z",
     "iopub.status.busy": "2025-06-11T07:10:13.174157Z",
     "iopub.status.idle": "2025-06-11T07:10:15.477390Z",
     "shell.execute_reply": "2025-06-11T07:10:15.476644Z",
     "shell.execute_reply.started": "2025-06-11T07:10:13.174495Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T06:45:21.579602Z",
     "iopub.status.busy": "2025-06-11T06:45:21.579236Z",
     "iopub.status.idle": "2025-06-11T06:45:45.214902Z",
     "shell.execute_reply": "2025-06-11T06:45:45.214035Z",
     "shell.execute_reply.started": "2025-06-11T06:45:21.579573Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./outputs\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    num_train_epochs=3\n",
    ")\n",
    "print(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T06:45:45.216945Z",
     "iopub.status.busy": "2025-06-11T06:45:45.216431Z",
     "iopub.status.idle": "2025-06-11T06:51:49.792489Z",
     "shell.execute_reply": "2025-06-11T06:51:49.791603Z",
     "shell.execute_reply.started": "2025-06-11T06:45:45.216926Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ======================== 1. Required Imports ======================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import re\n",
    "import copy\n",
    "import os\n",
    "from scipy import stats\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import (\n",
    "    T5EncoderModel, T5Tokenizer, T5PreTrainedModel, T5Config,\n",
    "    TrainingArguments, Trainer, set_seed\n",
    ")\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from torch.nn import MSELoss, CrossEntropyLoss, BCEWithLogitsLoss\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset\n",
    "\n",
    "# ======================== 2. Load GB1 Data ======================================\n",
    "import requests\n",
    "import zipfile\n",
    "from io import BytesIO\n",
    "\n",
    "url = 'https://github.com/J-SNACKKB/FLIP/raw/main/splits/gb1/splits.zip'\n",
    "response = requests.get(url)\n",
    "zip_file = zipfile.ZipFile(BytesIO(response.content))\n",
    "with zip_file.open('splits/three_vs_rest.csv') as file:\n",
    "    df = pd.read_csv(file)\n",
    "df = df.rename(columns={\"target\": \"label\"})\n",
    "if \"validation\" not in df.columns:\n",
    "    df[\"validation\"] = False\n",
    "\n",
    "# Ultra-small debug: only 8 samples per split, use for OOM testing\n",
    "N_TRAIN = 64\n",
    "N_VALID = 32\n",
    "N_TEST = 32\n",
    "\n",
    "my_train = df[(df[\"set\"]==\"train\") & (df[\"validation\"]!=True)][[\"sequence\", \"label\"]].reset_index(drop=True).iloc[:N_TRAIN]\n",
    "my_valid = df[(df[\"set\"]==\"train\") & (df[\"validation\"]==True)][[\"sequence\", \"label\"]].reset_index(drop=True).iloc[:N_VALID]\n",
    "my_test  = df[df[\"set\"]==\"test\"][[\"sequence\", \"label\"]].reset_index(drop=True).iloc[:N_TEST]\n",
    "\n",
    "print(my_train.head())\n",
    "\n",
    "# ======================== 3. Model & Tokenizer ========================\n",
    "class LoRAConfig:\n",
    "    def __init__(self):\n",
    "        self.lora_rank = 2\n",
    "        self.lora_init_scale = 0.01\n",
    "        self.lora_modules = r\".*SelfAttention|.*EncDecAttention\"\n",
    "        self.lora_layers = r\"q|k|v|o\"\n",
    "        self.trainable_param_names = r\".*layer_norm.*|.*lora_[ab].*\"\n",
    "        self.lora_scaling_rank = 1\n",
    "\n",
    "class LoRALinear(nn.Module):\n",
    "    def __init__(self, linear_layer, rank, scaling_rank, init_scale):\n",
    "        super().__init__()\n",
    "        self.in_features = linear_layer.in_features\n",
    "        self.out_features = linear_layer.out_features\n",
    "        self.rank = rank\n",
    "        self.scaling_rank = scaling_rank\n",
    "        self.weight = linear_layer.weight\n",
    "        self.bias = linear_layer.bias\n",
    "        if self.rank > 0:\n",
    "            self.lora_a = nn.Parameter(torch.randn(rank, linear_layer.in_features) * init_scale)\n",
    "            self.lora_b = nn.Parameter(torch.zeros(linear_layer.out_features, rank))\n",
    "        if self.scaling_rank:\n",
    "            self.multi_lora_a = nn.Parameter(\n",
    "                torch.ones(self.scaling_rank, linear_layer.in_features)\n",
    "                + torch.randn(self.scaling_rank, linear_layer.in_features) * init_scale\n",
    "            )\n",
    "            self.multi_lora_b = nn.Parameter(torch.ones(linear_layer.out_features, self.scaling_rank))\n",
    "\n",
    "    def forward(self, input):\n",
    "        weight = self.weight\n",
    "        if self.scaling_rank:\n",
    "            weight = weight * torch.matmul(self.multi_lora_b, self.multi_lora_a) / self.scaling_rank\n",
    "        if self.rank:\n",
    "            weight = weight + torch.matmul(self.lora_b, self.lora_a) / self.rank\n",
    "        return F.linear(input, weight, self.bias)\n",
    "\n",
    "def modify_with_lora(transformer, config):\n",
    "    for m_name, module in dict(transformer.named_modules()).items():\n",
    "        if re.fullmatch(config.lora_modules, m_name):\n",
    "            for c_name, layer in dict(module.named_children()).items():\n",
    "                if re.fullmatch(config.lora_layers, c_name):\n",
    "                    if isinstance(layer, nn.Linear):\n",
    "                        setattr(module, c_name,\n",
    "                            LoRALinear(layer, config.lora_rank, config.lora_scaling_rank, config.lora_init_scale))\n",
    "    return transformer\n",
    "\n",
    "class ClassConfig:\n",
    "    def __init__(self, dropout=0.2, num_labels=1):\n",
    "        self.dropout_rate = dropout\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "class T5EncoderClassificationHead(nn.Module):\n",
    "    def __init__(self, hidden_size, class_config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(hidden_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(class_config.dropout_rate)\n",
    "        self.out_proj = nn.Linear(hidden_size, class_config.num_labels)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = torch.mean(hidden_states, dim=1)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = torch.tanh(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.out_proj(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "class T5EncoderForSimpleSequenceClassification(T5PreTrainedModel):\n",
    "    def __init__(self, encoder_model, config: T5Config, class_config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = class_config.num_labels\n",
    "        self.config = config\n",
    "        self.encoder_model = encoder_model\n",
    "        self.dropout = nn.Dropout(class_config.dropout_rate) \n",
    "        self.classifier = T5EncoderClassificationHead(config.d_model, class_config)\n",
    "        self.post_init()\n",
    "        self.model_parallel = False\n",
    "        self.device_map = None\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        outputs = self.encoder_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            head_mask=head_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "        logits = self.classifier(hidden_states)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.config.problem_type is None:\n",
    "                if self.num_labels == 1:\n",
    "                    self.config.problem_type = \"regression\"\n",
    "                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
    "                    self.config.problem_type = \"single_label_classification\"\n",
    "                else:\n",
    "                    self.config.problem_type = \"multi_label_classification\"\n",
    "            if self.config.problem_type == \"regression\":\n",
    "                loss_fct = MSELoss()\n",
    "                loss = loss_fct(logits.squeeze(), labels.squeeze())\n",
    "            elif self.config.problem_type == \"single_label_classification\":\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            elif self.config.problem_type == \"multi_label_classification\":\n",
    "                loss_fct = BCEWithLogitsLoss()\n",
    "                loss = loss_fct(logits, labels)\n",
    "        if not return_dict:\n",
    "            output = (logits,) + (outputs[1:] if isinstance(outputs, tuple) else ())\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=getattr(outputs, \"hidden_states\", None),\n",
    "            attentions=getattr(outputs, \"attentions\", None),\n",
    "        )\n",
    "\n",
    "def PT5_classification_model(num_labels=1, half_precision=False):\n",
    "    if not half_precision:\n",
    "        encoder_model = T5EncoderModel.from_pretrained(\"Rostlab/prot_t5_xl_uniref50\")\n",
    "        tokenizer = T5Tokenizer.from_pretrained(\"Rostlab/prot_t5_xl_uniref50\", legacy=False)\n",
    "    elif half_precision and torch.cuda.is_available():\n",
    "        tokenizer = T5Tokenizer.from_pretrained('Rostlab/prot_t5_xl_half_uniref50-enc', do_lower_case=False, legacy=False)\n",
    "        encoder_model = T5EncoderModel.from_pretrained(\"Rostlab/prot_t5_xl_half_uniref50-enc\", torch_dtype=torch.float16).to(torch.device('cuda'))\n",
    "    else:\n",
    "        raise ValueError('Half precision can be run on GPU only.')\n",
    "\n",
    "    class_config = ClassConfig(num_labels=num_labels)\n",
    "    class_model = T5EncoderForSimpleSequenceClassification(encoder_model, encoder_model.config, class_config)\n",
    "    config = LoRAConfig()\n",
    "    class_model.encoder_model = modify_with_lora(class_model.encoder_model, config)\n",
    "    for name, param in class_model.encoder_model.named_parameters():\n",
    "        param.requires_grad = False\n",
    "    for name, param in class_model.named_parameters():\n",
    "        if re.fullmatch(config.trainable_param_names, name):\n",
    "            param.requires_grad = True\n",
    "    return class_model, tokenizer\n",
    "\n",
    "# ======================== 5. Custom Regression Head ==========================\n",
    "def set_seeds(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    set_seed(seed)\n",
    "\n",
    "def create_dataset(tokenizer, seqs, labels, max_length=128):  # Even smaller max_length\n",
    "    seqs_processed = [seq.replace('O', 'X').replace('B', 'X').replace('U', 'X').replace('Z', 'X') for seq in seqs]\n",
    "    seqs_spaced = [\" \".join(list(seq)) for seq in seqs_processed]\n",
    "    tokenized = tokenizer(seqs_spaced, max_length=max_length, padding='max_length', truncation=True)\n",
    "    dataset = Dataset.from_dict(tokenized)\n",
    "    dataset = dataset.add_column(\"labels\", labels)\n",
    "    return dataset\n",
    "\n",
    "# ======================== 6. Dataset & DataLoader ============================\n",
    "def train_per_protein(\n",
    "    train_df, \n",
    "    valid_df, \n",
    "    num_labels=1, \n",
    "    batch=3,   # SINGLE SAMPLE PER BATCH!\n",
    "    accum=1,\n",
    "    val_batch=3,\n",
    "    epochs=5,  # Only 1 epoch\n",
    "    lr=3e-4, \n",
    "    seed=42, \n",
    "    mixed=False, \n",
    "    gpu=0,     # Set to 0 to default to CPU if GPU fails\n",
    "    use_deepspeed=False  \n",
    "):\n",
    "    set_seeds(seed)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model, tokenizer = PT5_classification_model(num_labels=num_labels, half_precision=mixed)\n",
    "    model.to(device)\n",
    "    train_set = create_dataset(tokenizer, list(train_df[\"sequence\"]), list(train_df[\"label\"]), max_length=128)\n",
    "    valid_set = create_dataset(tokenizer, list(valid_df[\"sequence\"]), list(valid_df[\"label\"]), max_length=128)\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        output_dir=\"./outputs\",\n",
    "        learning_rate=lr,\n",
    "        per_device_train_batch_size=batch,\n",
    "        per_device_eval_batch_size=val_batch,\n",
    "        gradient_accumulation_steps=accum,\n",
    "        num_train_epochs=epochs,\n",
    "        seed=seed,\n",
    "        fp16=False,  # No fp16 for safety (try enabling later if you want)\n",
    "        dataloader_num_workers=0,\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        predictions, labels = eval_pred\n",
    "        predictions = np.squeeze(predictions)\n",
    "        labels = np.squeeze(labels)\n",
    "        if predictions.ndim == 0:\n",
    "            predictions = np.array([predictions])\n",
    "        if labels.ndim == 0:\n",
    "            labels = np.array([labels])\n",
    "        correlation = stats.spearmanr(predictions, labels).correlation\n",
    "        if np.isnan(correlation):\n",
    "            correlation = 0.0\n",
    "        return {\"spearmanr\": correlation}\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_set,\n",
    "        eval_dataset=valid_set,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    print(\"Starting training...\")\n",
    "    trainer.train()\n",
    "    print(\"Training completed!\")\n",
    "    return tokenizer, model, trainer.state.log_history\n",
    "\n",
    "# ======================== 7. Training Loop (Simple) ==========================\n",
    "print(\"Starting PT5 training...\")\n",
    "\n",
    "tokenizer, model, history = train_per_protein(\n",
    "    my_train, \n",
    "    my_valid, \n",
    "    num_labels=1, \n",
    "    batch=3,     # OOM protection: batch size 1\n",
    "    accum=1,\n",
    "    val_batch=3,\n",
    "    epochs=5,    # Just 1 epoch for quick run\n",
    "    lr=3e-4,\n",
    "    seed=42,\n",
    "    mixed=False, \n",
    "    use_deepspeed=False \n",
    ")\n",
    "\n",
    "# ======================== 8. Plot Results ==========================\n",
    "loss = [x['loss'] for x in history if 'loss' in x]\n",
    "val_loss = [x['eval_loss'] for x in history if 'eval_loss' in x]\n",
    "metric = [x['eval_spearmanr'] for x in history if 'eval_spearmanr' in x]\n",
    "epochs_list = [x['epoch'] for x in history if 'loss' in x]\n",
    "\n",
    "if len(loss) > 0:\n",
    "    fig, ax1 = plt.subplots(figsize=(8, 4))\n",
    "    ax2 = ax1.twinx()\n",
    "    line1 = ax1.plot(epochs_list, loss, label='train_loss')\n",
    "    line2 = ax1.plot(epochs_list, val_loss, label='val_loss') if len(val_loss) > 0 else []\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    line3 = ax2.plot(epochs_list, metric, color='red', label='val_metric') if len(metric) > 0 else []\n",
    "    ax2.set_ylabel('Spearman r')\n",
    "    ax2.set_ylim([0, 1])\n",
    "    lines = line1 + line2 + line3\n",
    "    labels = [line.get_label() for line in lines]\n",
    "    ax1.legend(lines, labels, loc='lower left')\n",
    "    plt.title(\"Training History\")\n",
    "    plt.show()\n",
    "\n",
    "# ======================== 9. Save & Reload Model (LoRA weights only) ==========================\n",
    "def save_model(model, filepath):\n",
    "    non_frozen_params = {n: p for n, p in model.named_parameters() if p.requires_grad}\n",
    "    torch.save(non_frozen_params, filepath)\n",
    "    print(f\"Model saved to {filepath}\")\n",
    "\n",
    "def load_model(filepath, num_labels=1, mixed=False):\n",
    "    model, tokenizer = PT5_classification_model(num_labels=num_labels, half_precision=mixed)\n",
    "    non_frozen_params = torch.load(filepath, map_location='cpu')\n",
    "    for name, param in model.named_parameters():\n",
    "        if name in non_frozen_params:\n",
    "            param.data = non_frozen_params[name].data.clone()\n",
    "    print(f\"Model loaded from {filepath}\")\n",
    "    return tokenizer, model\n",
    "\n",
    "save_model(model, \"./PT5_GB1_finetuned.pth\")\n",
    "tokenizer_reload, model_reload = load_model(\"./PT5_GB1_finetuned.pth\", num_labels=1, mixed=False)\n",
    "\n",
    "# ======================== 10. Inference on Test Set ==========================\n",
    "print(\"Running inference on test set...\")\n",
    "\n",
    "test_set = create_dataset(tokenizer_reload, list(my_test[\"sequence\"]), list(my_test[\"label\"]), max_length=128)\n",
    "test_set = test_set.with_format(\"torch\")\n",
    "test_dataloader = DataLoader(test_set, batch_size=3, shuffle=False)   # batch_size 1 for OOM safety\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_reload.to(device)\n",
    "model_reload.eval()\n",
    "\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_dataloader, desc=\"Testing\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        logits = model_reload(input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "        batch_predictions = logits.cpu().numpy().squeeze()\n",
    "        if batch_predictions.ndim == 0:\n",
    "            batch_predictions = [batch_predictions.item()]\n",
    "        else:\n",
    "            batch_predictions = batch_predictions.tolist()\n",
    "        predictions.extend(batch_predictions)\n",
    "\n",
    "test_correlation = stats.spearmanr(predictions, my_test['label']).correlation\n",
    "print(f\"Test Spearman r: {test_correlation:.4f}\")\n",
    "\n",
    "\n",
    "# ======================== 11. Plot Test Predictions ==========================\n",
    "import seaborn as sns\n",
    "\n",
    "true_labels = my_test['label'].values\n",
    "predictions_arr = np.array(predictions)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.scatterplot(x=true_labels, y=predictions_arr, s=80, color='royalblue', edgecolor='k')\n",
    "plt.plot([true_labels.min(), true_labels.max()],\n",
    "         [true_labels.min(), true_labels.max()],\n",
    "         'r--', lw=2, label='Identity (y=x)')\n",
    "plt.xlabel(\"True Fitness (Label)\")\n",
    "plt.ylabel(\"Predicted Fitness\")\n",
    "plt.title(f\"Test Set Predictions vs True (Spearman r = {test_correlation:.3f})\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-06-11T07:27:26.324Z",
     "iopub.execute_input": "2025-06-11T07:23:07.544122Z",
     "iopub.status.busy": "2025-06-11T07:23:07.543853Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ======================== 1. Required Imports ======================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "from scipy import stats\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import (\n",
    "    T5EncoderModel, T5Tokenizer, T5PreTrainedModel, T5Config,\n",
    "    TrainingArguments, Trainer, set_seed\n",
    ")\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from torch.nn import MSELoss\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset\n",
    "\n",
    "# ======================== 2. Load GB1 Data ======================================\n",
    "import requests\n",
    "import zipfile\n",
    "from io import BytesIO\n",
    "\n",
    "url = 'https://github.com/J-SNACKKB/FLIP/raw/main/splits/gb1/splits.zip'\n",
    "response = requests.get(url)\n",
    "zip_file = zipfile.ZipFile(BytesIO(response.content))\n",
    "with zip_file.open('splits/three_vs_rest.csv') as file:\n",
    "    df = pd.read_csv(file)\n",
    "df = df.rename(columns={\"target\": \"label\"})\n",
    "if \"validation\" not in df.columns:\n",
    "    df[\"validation\"] = False\n",
    "\n",
    "# Use ALL data\n",
    "my_train = df[(df[\"set\"]==\"train\") & (df[\"validation\"]!=True)][[\"sequence\", \"label\"]].reset_index(drop=True)\n",
    "my_valid = df[(df[\"set\"]==\"train\") & (df[\"validation\"]==True)][[\"sequence\", \"label\"]].reset_index(drop=True)\n",
    "my_test  = df[df[\"set\"]==\"test\"][[\"sequence\", \"label\"]].reset_index(drop=True)\n",
    "\n",
    "print(f\"Train: {len(my_train)}, Val: {len(my_valid)}, Test: {len(my_test)}\")\n",
    "print(my_train.head())\n",
    "\n",
    "# ======================== 3. Model & Tokenizer (improved) ========================\n",
    "class LoRAConfig:\n",
    "    def __init__(self):\n",
    "        self.lora_rank = 2\n",
    "        self.lora_init_scale = 0.01\n",
    "        self.lora_modules = r\".*SelfAttention|.*EncDecAttention\"\n",
    "        self.lora_layers = r\"q|k|v|o\"\n",
    "        self.trainable_param_names = r\".*layer_norm.*|.*lora_[ab].*|.*classifier.*|.*block\\.23.*\"  # Unfreeze head & last block\n",
    "        self.lora_scaling_rank = 1\n",
    "\n",
    "class LoRALinear(nn.Module):\n",
    "    def __init__(self, linear_layer, rank, scaling_rank, init_scale):\n",
    "        super().__init__()\n",
    "        self.in_features = linear_layer.in_features\n",
    "        self.out_features = linear_layer.out_features\n",
    "        self.rank = rank\n",
    "        self.scaling_rank = scaling_rank\n",
    "        self.weight = linear_layer.weight\n",
    "        self.bias = linear_layer.bias\n",
    "        if self.rank > 0:\n",
    "            self.lora_a = nn.Parameter(torch.randn(rank, linear_layer.in_features) * init_scale)\n",
    "            self.lora_b = nn.Parameter(torch.zeros(linear_layer.out_features, rank))\n",
    "        if self.scaling_rank:\n",
    "            self.multi_lora_a = nn.Parameter(\n",
    "                torch.ones(self.scaling_rank, linear_layer.in_features)\n",
    "                + torch.randn(self.scaling_rank, linear_layer.in_features) * init_scale\n",
    "            )\n",
    "            self.multi_lora_b = nn.Parameter(torch.ones(linear_layer.out_features, self.scaling_rank))\n",
    "\n",
    "    def forward(self, input):\n",
    "        weight = self.weight\n",
    "        if self.scaling_rank:\n",
    "            weight = weight * torch.matmul(self.multi_lora_b, self.multi_lora_a) / self.scaling_rank\n",
    "        if self.rank:\n",
    "            weight = weight + torch.matmul(self.lora_b, self.lora_a) / self.rank\n",
    "        return F.linear(input, weight, self.bias)\n",
    "\n",
    "def modify_with_lora(transformer, config):\n",
    "    for m_name, module in dict(transformer.named_modules()).items():\n",
    "        if re.fullmatch(config.lora_modules, m_name):\n",
    "            for c_name, layer in dict(module.named_children()).items():\n",
    "                if re.fullmatch(config.lora_layers, c_name):\n",
    "                    if isinstance(layer, nn.Linear):\n",
    "                        setattr(module, c_name,\n",
    "                            LoRALinear(layer, config.lora_rank, config.lora_scaling_rank, config.lora_init_scale))\n",
    "    return transformer\n",
    "\n",
    "class ClassConfig:\n",
    "    def __init__(self, dropout=0.2, num_labels=1):\n",
    "        self.dropout_rate = dropout\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "class T5EncoderClassificationHead(nn.Module):\n",
    "    def __init__(self, hidden_size, class_config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(hidden_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(class_config.dropout_rate)\n",
    "        self.out_proj = nn.Linear(hidden_size, class_config.num_labels)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        # Instead of mean pooling, try [first] token\n",
    "        pooled = hidden_states[:, 0, :]  # first token ([CLS]-like)\n",
    "        pooled = self.dropout(pooled)\n",
    "        pooled = self.dense(pooled)\n",
    "        pooled = torch.tanh(pooled)\n",
    "        pooled = self.dropout(pooled)\n",
    "        logits = self.out_proj(pooled)\n",
    "        return logits\n",
    "\n",
    "class T5EncoderForSimpleSequenceRegression(T5PreTrainedModel):\n",
    "    def __init__(self, encoder_model, config: T5Config, class_config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = class_config.num_labels\n",
    "        self.config = config\n",
    "        self.encoder_model = encoder_model\n",
    "        self.dropout = nn.Dropout(class_config.dropout_rate)\n",
    "        self.classifier = T5EncoderClassificationHead(config.d_model, class_config)\n",
    "        self.post_init()\n",
    "        self.model_parallel = False\n",
    "        self.device_map = None\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        outputs = self.encoder_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            head_mask=head_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "        logits = self.classifier(hidden_states)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = MSELoss()\n",
    "            loss = loss_fct(logits.squeeze(), labels.squeeze())\n",
    "        if not return_dict:\n",
    "            output = (logits,) + (outputs[1:] if isinstance(outputs, tuple) else ())\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=getattr(outputs, \"hidden_states\", None),\n",
    "            attentions=getattr(outputs, \"attentions\", None),\n",
    "        )\n",
    "\n",
    "def PT5_regression_model(num_labels=1, half_precision=False):\n",
    "    if not half_precision:\n",
    "        encoder_model = T5EncoderModel.from_pretrained(\"Rostlab/prot_t5_xl_uniref50\")\n",
    "        tokenizer = T5Tokenizer.from_pretrained(\"Rostlab/prot_t5_xl_uniref50\", legacy=False)\n",
    "    elif half_precision and torch.cuda.is_available():\n",
    "        tokenizer = T5Tokenizer.from_pretrained('Rostlab/prot_t5_xl_half_uniref50-enc', do_lower_case=False, legacy=False)\n",
    "        encoder_model = T5EncoderModel.from_pretrained(\"Rostlab/prot_t5_xl_half_uniref50-enc\", torch_dtype=torch.float16).to(torch.device('cuda'))\n",
    "    else:\n",
    "        raise ValueError('Half precision can be run on GPU only.')\n",
    "\n",
    "    class_config = ClassConfig(num_labels=num_labels)\n",
    "    class_model = T5EncoderForSimpleSequenceRegression(encoder_model, encoder_model.config, class_config)\n",
    "    config = LoRAConfig()\n",
    "    class_model.encoder_model = modify_with_lora(class_model.encoder_model, config)\n",
    "    # Unfreeze head, last block, and LoRA\n",
    "    for name, param in class_model.named_parameters():\n",
    "        if re.fullmatch(config.trainable_param_names, name):\n",
    "            param.requires_grad = True\n",
    "        else:\n",
    "            param.requires_grad = False\n",
    "    return class_model, tokenizer\n",
    "\n",
    "def set_seeds(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    set_seed(seed)\n",
    "\n",
    "def create_dataset(tokenizer, seqs, labels, max_length=128):\n",
    "    seqs_processed = [seq.replace('O', 'X').replace('B', 'X').replace('U', 'X').replace('Z', 'X') for seq in seqs]\n",
    "    seqs_spaced = [\" \".join(list(seq)) for seq in seqs_processed]\n",
    "    tokenized = tokenizer(seqs_spaced, max_length=max_length, padding='max_length', truncation=True)\n",
    "    dataset = Dataset.from_dict(tokenized)\n",
    "    dataset = dataset.add_column(\"labels\", labels)\n",
    "    return dataset\n",
    "\n",
    "def train_per_protein(\n",
    "    train_df, \n",
    "    valid_df, \n",
    "    num_labels=1, \n",
    "    batch=4,\n",
    "    accum=1,\n",
    "    val_batch=16,\n",
    "    epochs=10,  # More epochs for higher accuracy\n",
    "    lr=2e-4,\n",
    "    wd=0.01,\n",
    "    seed=42, \n",
    "    mixed=False,\n",
    "):\n",
    "    set_seeds(seed)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model, tokenizer = PT5_regression_model(num_labels=num_labels, half_precision=mixed)\n",
    "    model.to(device)\n",
    "    train_set = create_dataset(tokenizer, list(train_df[\"sequence\"]), list(train_df[\"label\"]), max_length=128)\n",
    "    valid_set = create_dataset(tokenizer, list(valid_df[\"sequence\"]), list(valid_df[\"label\"]), max_length=128)\n",
    "\n",
    "    args = TrainingArguments(\n",
    "    output_dir=\"./outputs\",\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=batch,\n",
    "    per_device_eval_batch_size=val_batch,\n",
    "    gradient_accumulation_steps=accum,\n",
    "    num_train_epochs=epochs,\n",
    "    weight_decay=wd,\n",
    "    learning_rate=lr,\n",
    "    seed=seed,\n",
    "    fp16=False,\n",
    "    dataloader_num_workers=2,\n",
    "    report_to=\"none\",\n",
    "    save_total_limit=1,\n",
    "    # Remove all evaluation_strategy, save_strategy, load_best_model_at_end, metric_for_best_model, greater_is_better\n",
    "    )\n",
    "\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        predictions, labels = eval_pred\n",
    "        predictions = np.squeeze(predictions)\n",
    "        labels = np.squeeze(labels)\n",
    "        if predictions.ndim == 0:\n",
    "            predictions = np.array([predictions])\n",
    "        if labels.ndim == 0:\n",
    "            labels = np.array([labels])\n",
    "        correlation = stats.spearmanr(predictions, labels).correlation\n",
    "        if np.isnan(correlation):\n",
    "            correlation = 0.0\n",
    "        return {\"spearmanr\": correlation}\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_set,\n",
    "        eval_dataset=valid_set,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    print(\"Starting training...\")\n",
    "    trainer.train()\n",
    "    print(\"Training completed!\")\n",
    "    return tokenizer, model, trainer.state.log_history\n",
    "\n",
    "# ======================== 7. Training Loop (Simple) ==========================\n",
    "print(\"Starting PT5 regression training...\")\n",
    "\n",
    "tokenizer, model, history = train_per_protein(\n",
    "    my_train,\n",
    "    my_valid,\n",
    "    num_labels=1,\n",
    "    batch=4,\n",
    "    accum=1,\n",
    "    val_batch=16,\n",
    "    epochs=10,\n",
    "    lr=2e-4,\n",
    "    wd=0.01,\n",
    "    seed=42,\n",
    "    mixed=False,\n",
    ")\n",
    "\n",
    "# ======================== 8. Plot Results ==========================\n",
    "loss = [x['loss'] for x in history if 'loss' in x]\n",
    "val_loss = [x['eval_loss'] for x in history if 'eval_loss' in x]\n",
    "metric = [x['eval_spearmanr'] for x in history if 'eval_spearmanr' in x]\n",
    "epochs_list = [x['epoch'] for x in history if 'loss' in x]\n",
    "\n",
    "if len(loss) > 0:\n",
    "    fig, ax1 = plt.subplots(figsize=(8, 4))\n",
    "    ax2 = ax1.twinx()\n",
    "    line1 = ax1.plot(epochs_list, loss, label='train_loss')\n",
    "    line2 = ax1.plot(epochs_list, val_loss, label='val_loss') if len(val_loss) > 0 else []\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    line3 = ax2.plot(epochs_list, metric, color='red', label='val_metric') if len(metric) > 0 else []\n",
    "    ax2.set_ylabel('Spearman r')\n",
    "    ax2.set_ylim([0, 1])\n",
    "    lines = line1 + line2 + line3\n",
    "    labels = [line.get_label() for line in lines]\n",
    "    ax1.legend(lines, labels, loc='lower left')\n",
    "    plt.title(\"Training History\")\n",
    "    plt.show()\n",
    "\n",
    "# ====================== 9. Save & Reload Model (LoRA weights only) =========================\n",
    "def save_model(model, filepath):\n",
    "    non_frozen_params = {n: p for n, p in model.named_parameters() if p.requires_grad}\n",
    "    torch.save(non_frozen_params, filepath)\n",
    "    print(f\"Model saved to {filepath}\")\n",
    "\n",
    "def load_model(filepath, num_labels=1, mixed=False):\n",
    "    model, tokenizer = PT5_regression_model(num_labels=num_labels, half_precision=mixed)\n",
    "    non_frozen_params = torch.load(filepath, map_location='cpu')\n",
    "    for name, param in model.named_parameters():\n",
    "        if name in non_frozen_params:\n",
    "            param.data = non_frozen_params[name].data.clone()\n",
    "    print(f\"Model loaded from {filepath}\")\n",
    "    return tokenizer, model\n",
    "\n",
    "save_model(model, \"./PT5_GB1_finetuned.pth\")\n",
    "tokenizer_reload, model_reload = load_model(\"./PT5_GB1_finetuned.pth\", num_labels=1, mixed=False)\n",
    "\n",
    "# ======================== 10. Inference on Test Set ==========================\n",
    "print(\"Running inference on test set...\")\n",
    "\n",
    "test_set = create_dataset(tokenizer_reload, list(my_test[\"sequence\"]), list(my_test[\"label\"]), max_length=128)\n",
    "test_set = test_set.with_format(\"torch\")\n",
    "test_dataloader = DataLoader(test_set, batch_size=8, shuffle=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_reload.to(device)\n",
    "model_reload.eval()\n",
    "\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_dataloader, desc=\"Testing\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        logits = model_reload(input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "        batch_predictions = logits.cpu().numpy().squeeze()\n",
    "        if batch_predictions.ndim == 0:\n",
    "            batch_predictions = [batch_predictions.item()]\n",
    "        else:\n",
    "            batch_predictions = batch_predictions.tolist()\n",
    "        predictions.extend(batch_predictions)\n",
    "\n",
    "true_labels = my_test['label'].values\n",
    "predictions_arr = np.array(predictions)\n",
    "test_correlation = stats.spearmanr(predictions_arr, true_labels).correlation\n",
    "pearson_corr = stats.pearsonr(predictions_arr, true_labels)[0]\n",
    "\n",
    "print(f\"Test Spearman r: {test_correlation:.4f}\")\n",
    "print(f\"Test Pearson r:  {pearson_corr:.4f}\")\n",
    "print(f\"Variance of predictions: {np.var(predictions_arr):.6f}\")\n",
    "\n",
    "# Print table for manual inspection\n",
    "print(\"\\nTrue vs Predicted on Test Set:\")\n",
    "print(pd.DataFrame({\n",
    "    \"True\": true_labels,\n",
    "    \"Pred\": predictions_arr\n",
    "}).head(20))\n",
    "\n",
    "# ======================== 11. Plot Test Predictions ==========================\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.scatterplot(x=true_labels, y=predictions_arr, s=80, color='royalblue', edgecolor='k')\n",
    "plt.plot([true_labels.min(), true_labels.max()],\n",
    "         [true_labels.min(), true_labels.max()],\n",
    "         'r--', lw=2, label='Identity (y=x)')\n",
    "plt.xlabel(\"True Fitness (Label)\")\n",
    "plt.ylabel(\"Predicted Fitness\")\n",
    "plt.title(f\"Test Set Predictions vs True (Spearman r = {test_correlation:.3f})\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
