{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-06-11T08:08:53.808575Z",
     "iopub.status.busy": "2025-06-11T08:08:53.808307Z",
     "iopub.status.idle": "2025-06-11T08:10:16.631666Z",
     "shell.execute_reply": "2025-06-11T08:10:16.630995Z",
     "shell.execute_reply.started": "2025-06-11T08:08:53.808555Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Install necessary packages\n",
    "!pip install torch transformers scikit-learn pandas numpy biopython peft bitsandbytes requests optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T08:10:16.633278Z",
     "iopub.status.busy": "2025-06-11T08:10:16.633004Z",
     "iopub.status.idle": "2025-06-11T08:11:02.831803Z",
     "shell.execute_reply": "2025-06-11T08:11:02.831050Z",
     "shell.execute_reply.started": "2025-06-11T08:10:16.633253Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Install necessary tools in Google Colab\n",
    "!apt-get install -y mafft\n",
    "!apt-get install -y hmmer\n",
    "!wget https://github.com/soedinglab/MMseqs2/releases/download/17-b804f/mmseqs-linux-gpu.tar.gz\n",
    "!tar xvf mmseqs-linux-gpu.tar.gz\n",
    "!chmod +x mmseqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T08:11:02.833072Z",
     "iopub.status.busy": "2025-06-11T08:11:02.832856Z",
     "iopub.status.idle": "2025-06-11T08:11:29.169743Z",
     "shell.execute_reply": "2025-06-11T08:11:29.169144Z",
     "shell.execute_reply.started": "2025-06-11T08:11:02.833047Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import requests\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, Trainer, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "from Bio import SeqIO\n",
    "from Bio import AlignIO\n",
    "import optuna\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import time\n",
    "import xml.etree.ElementTree as ET\n",
    "from scipy.stats import pearsonr\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T08:11:29.171665Z",
     "iopub.status.busy": "2025-06-11T08:11:29.171089Z",
     "iopub.status.idle": "2025-06-11T08:14:53.799023Z",
     "shell.execute_reply": "2025-06-11T08:14:53.798238Z",
     "shell.execute_reply.started": "2025-06-11T08:11:29.171644Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import requests, time, subprocess\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "from Bio import AlignIO\n",
    "\n",
    "WT_SEQUENCE = \"MRHGDISSSNDTVGVAVVNYKMPRLHTAAEVLDNARKIAEMIVGMKQGLPGMDLVVFPEYSLQGIMYDPAEMMETAVAIPGEETEIFSRACRKANVWGVFSLTGERHEEHPRKAPYNTLVLIDNNGEIVQKYRKIIPWCPIEGWYPGGQTYVSEGPKGMKISLIICDDGNYPEIWRDCAMKGAELIVRCQGYMYPAKDQQVMMAKAMAWANNCYVAVANAAGFDGVYSYFGHSAIIGFDGRTLGECGEEEMGIQYAQLSLSQIRDARANDQSQNHLFKILHRGYSGLQASGDGDRGLAECPFEFYRTWVTDAEKARENVERLTRSTTGVAQCPVGRLPYEG\"\n",
    "BLAST_URL = \"https://blast.ncbi.nlm.nih.gov/Blast.cgi\"\n",
    "\n",
    "# ======= Step 1: BLAST API for Homologs =======\n",
    "def run_blast_search(wt_sequence, identity_threshold=90.0, max_retries=30, sleep_time=10, min_length=100, hitlist_size=300):\n",
    "    \"\"\"Run BLAST and extract homologous sequences below a given identity threshold.\"\"\"\n",
    "    params = {\n",
    "        \"CMD\": \"Put\",\n",
    "        \"PROGRAM\": \"blastp\",\n",
    "        \"DATABASE\": \"nr\",\n",
    "        \"QUERY\": wt_sequence,\n",
    "        \"FORMAT_TYPE\": \"XML\",\n",
    "        \"EXPECT\": \"1e-2\",\n",
    "        \"HITLIST_SIZE\": str(hitlist_size)\n",
    "    }\n",
    "    response = requests.post(BLAST_URL, data=params)\n",
    "    response.raise_for_status()\n",
    "    response_text = response.text\n",
    "    if \"RID = \" not in response_text:\n",
    "        raise Exception(\"No RID found in BLAST response.\")\n",
    "    rid = response_text.split(\"RID = \")[-1].split(\"\\n\")[0].strip()\n",
    "    print(f\"BLAST RID: {rid}\")\n",
    "\n",
    "    # Wait for completion\n",
    "    for attempt in range(max_retries):\n",
    "        status = requests.get(BLAST_URL, params={\"CMD\":\"Get\", \"FORMAT_OBJECT\":\"SearchInfo\", \"RID\":rid})\n",
    "        if \"Status=READY\" in status.text:\n",
    "            print(\"BLAST complete.\")\n",
    "            break\n",
    "        print(f\"Waiting... {attempt+1}/{max_retries}\")\n",
    "        time.sleep(sleep_time)\n",
    "    else:\n",
    "        raise Exception(\"BLAST timed out\")\n",
    "\n",
    "    # Download results\n",
    "    result = requests.get(BLAST_URL, params={\"CMD\":\"Get\", \"FORMAT_TYPE\":\"XML\", \"RID\":rid})\n",
    "    result.raise_for_status()\n",
    "    root = ET.fromstring(result.text)\n",
    "    seqs = []\n",
    "    for hit in root.findall(\".//Hit\"):\n",
    "        for hsp in hit.findall(\".//Hsp\"):\n",
    "            hseq_elem = hsp.find(\"Hsp_hseq\")\n",
    "            identity_elem = hsp.find(\"Hsp_identity\")\n",
    "            align_len_elem = hsp.find(\"Hsp_align-len\")\n",
    "            if hseq_elem is not None and identity_elem is not None and align_len_elem is not None:\n",
    "                hseq = hseq_elem.text.strip()\n",
    "                identity = int(identity_elem.text)\n",
    "                align_len = int(align_len_elem.text)\n",
    "                identity_pct = 100 * identity / align_len\n",
    "                if identity_pct < identity_threshold and len(hseq) > min_length:\n",
    "                    seqs.append(hseq)\n",
    "    seqs = [wt_sequence] + list({s for s in seqs if s != wt_sequence})  # unique, include WT\n",
    "    print(f\"Total homologs: {len(seqs)}\")\n",
    "    # Save to FASTA\n",
    "    with open(\"msa_input.fasta\", \"w\") as f:\n",
    "        for i, s in enumerate(seqs):\n",
    "            f.write(f\">seq{i}\\n{s}\\n\")\n",
    "    return \"msa_input.fasta\"\n",
    "\n",
    "# ======= Step 2: Align with MAFFT =======\n",
    "def run_mafft(input_fasta, output_fasta=\"msa_aligned.fasta\"):\n",
    "    print(f\"Running MAFFT alignment...\")\n",
    "    cmd = f\"mafft --auto {input_fasta} > {output_fasta}\"\n",
    "    subprocess.run(cmd, shell=True, check=True)\n",
    "    print(f\"Alignment written: {output_fasta}\")\n",
    "    return output_fasta\n",
    "\n",
    "# ======= Step 3: Calculate Henikoff Weights =======\n",
    "def henikoff_weights(msa_file, format=\"fasta\"):\n",
    "    alignment = AlignIO.read(msa_file, format)\n",
    "    n_seq = len(alignment)\n",
    "    aln_len = alignment.get_alignment_length()\n",
    "    weights = np.zeros(n_seq)\n",
    "    for pos in range(aln_len):\n",
    "        aa_counts = {}\n",
    "        for record in alignment:\n",
    "            aa = record.seq[pos]\n",
    "            if aa not in aa_counts:\n",
    "                aa_counts[aa] = 0\n",
    "            aa_counts[aa] += 1\n",
    "        n_types = len(aa_counts)\n",
    "        for i, record in enumerate(alignment):\n",
    "            aa = record.seq[pos]\n",
    "            weights[i] += 1.0 / (n_types * aa_counts[aa])\n",
    "    weights /= weights.sum()\n",
    "    return weights\n",
    "\n",
    "# ======= (Optional) Jackhmmer/MMseqs2 integration (not changed here) =======\n",
    "\n",
    "# ==== MAIN WORKFLOW ====\n",
    "method = \"blast\"  # \"jackhmmer\" or \"mmseqs2\" possible if implemented\n",
    "\n",
    "if method == \"blast\":\n",
    "    msa_input = run_blast_search(WT_SEQUENCE, identity_threshold=90.0, hitlist_size=500)\n",
    "elif method == \"jackhmmer\":\n",
    "    msa_input = run_jackhmmer_search(WT_SEQUENCE)\n",
    "elif method == \"mmseqs2\":\n",
    "    msa_input = run_mmseqs2_search(WT_SEQUENCE)\n",
    "else:\n",
    "    raise ValueError(\"Invalid method chosen. Please select 'blast', 'jackhmmer', or 'mmseqs2'.\")\n",
    "\n",
    "msa_aligned = run_mafft(msa_input)\n",
    "\n",
    "weights = henikoff_weights(msa_aligned, \"fasta\")\n",
    "print(\"Sequence weights:\", weights)\n",
    "np.save(\"msa_weights.npy\", weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T08:14:53.800466Z",
     "iopub.status.busy": "2025-06-11T08:14:53.800079Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ======================== 1. Required Imports ======================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "from scipy import stats\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    T5EncoderModel, T5Tokenizer, T5PreTrainedModel, T5Config,\n",
    "    TrainingArguments, Trainer, set_seed\n",
    ")\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from torch.nn import MSELoss\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset\n",
    "\n",
    "# ======================== 2. Load and preprocess data =============================\n",
    "# Use SSM fitness data (from figshare)\n",
    "url = \"https://figshare.com/ndownloader/files/7337543\"\n",
    "df = pd.read_csv(url, sep='\\t')\n",
    "df.rename(columns={'mutation': 'mutation_string', 'normalized_fitness': 'fitness'}, inplace=True)\n",
    "df['fitness'] = pd.to_numeric(df['fitness'], errors='coerce')\n",
    "df.dropna(subset=['fitness'], inplace=True)\n",
    "\n",
    "# --- Add weights ---\n",
    "# Here, we'll add inverse frequency as weights, just as an example\n",
    "msa_weights = np.load(\"msa_weights.npy\")\n",
    "wt_weight = msa_weights[0]  # First sequence is the wild-type\n",
    "\n",
    "# Add this weight to every row in your DataFrame:\n",
    "df['weight'] = wt_weight\n",
    "\n",
    "# --- Generate mutated sequences from wildtype + mutation string ---\n",
    "WT_SEQUENCE = \"MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKFICTTGKLPVPWPTLVTTLTYGVQCFSRYPDHMKQHDFFKSAMPEGYVQERTIFFKDDGNYKTRAEVKFEGDTLVNRIELKGIDFKEDGNILGHKLEYNYNSHNVYIMADKQKNGIKVNFKIRHNIEDGSVQLADHYQQNTPIGDGPVLLPDNHYLSTQSALSKDPNEKRDHMVLLEFVTAAGITHGMDELYK\"\n",
    "def generate_mutated_sequence(wt_sequence, mutation_string):\n",
    "    seq_list = list(wt_sequence)\n",
    "    mutations = mutation_string.split(',')\n",
    "    for mut in mutations:\n",
    "        mut = mut.strip()\n",
    "        if len(mut) >= 3 and mut[1:-1].isdigit():\n",
    "            pos = int(mut[1:-1]) - 1\n",
    "            if 0 <= pos < len(seq_list) and mut[-1] != '*':\n",
    "                seq_list[pos] = mut[-1]\n",
    "    return ''.join(seq_list)\n",
    "df['sequence'] = df['mutation_string'].apply(lambda x: generate_mutated_sequence(WT_SEQUENCE, x))\n",
    "\n",
    "# Format for T5: whitespace between each AA\n",
    "def format_protein_sequence(sequence):\n",
    "    return ' '.join(list(sequence))\n",
    "df['sequence'] = df['sequence'].apply(format_protein_sequence)\n",
    "\n",
    "# ======================== 3. Train/val/test split ================================\n",
    "# (If you want stratify: stratify=df['fitness'] > df['fitness'].median())\n",
    "from sklearn.model_selection import train_test_split\n",
    "my_train, temp = train_test_split(df, test_size=0.3, random_state=42)\n",
    "my_valid, my_test = train_test_split(temp, test_size=0.5, random_state=42)\n",
    "print(f\"Train: {len(my_train)}, Val: {len(my_valid)}, Test: {len(my_test)}\")\n",
    "print(my_train[['sequence', 'fitness', 'weight']].head())\n",
    "\n",
    "# ======================== 4. Model & Tokenizer (improved) ========================\n",
    "class LoRAConfig:\n",
    "    def __init__(self):\n",
    "        self.lora_rank = 2\n",
    "        self.lora_init_scale = 0.01\n",
    "        self.lora_modules = r\".*SelfAttention|.*EncDecAttention\"\n",
    "        self.lora_layers = r\"q|k|v|o\"\n",
    "        self.trainable_param_names = r\".*layer_norm.*|.*lora_[ab].*|.*classifier.*|.*block\\.23.*\"  # Unfreeze head & last block\n",
    "        self.lora_scaling_rank = 1\n",
    "\n",
    "class LoRALinear(nn.Module):\n",
    "    def __init__(self, linear_layer, rank, scaling_rank, init_scale):\n",
    "        super().__init__()\n",
    "        self.in_features = linear_layer.in_features\n",
    "        self.out_features = linear_layer.out_features\n",
    "        self.rank = rank\n",
    "        self.scaling_rank = scaling_rank\n",
    "        self.weight = linear_layer.weight\n",
    "        self.bias = linear_layer.bias\n",
    "        if self.rank > 0:\n",
    "            self.lora_a = nn.Parameter(torch.randn(rank, linear_layer.in_features) * init_scale)\n",
    "            self.lora_b = nn.Parameter(torch.zeros(linear_layer.out_features, rank))\n",
    "        if self.scaling_rank:\n",
    "            self.multi_lora_a = nn.Parameter(\n",
    "                torch.ones(self.scaling_rank, linear_layer.in_features)\n",
    "                + torch.randn(self.scaling_rank, linear_layer.in_features) * init_scale\n",
    "            )\n",
    "            self.multi_lora_b = nn.Parameter(torch.ones(linear_layer.out_features, self.scaling_rank))\n",
    "    def forward(self, input):\n",
    "        weight = self.weight\n",
    "        if self.scaling_rank:\n",
    "            weight = weight * torch.matmul(self.multi_lora_b, self.multi_lora_a) / self.scaling_rank\n",
    "        if self.rank:\n",
    "            weight = weight + torch.matmul(self.lora_b, self.lora_a) / self.rank\n",
    "        return F.linear(input, weight, self.bias)\n",
    "\n",
    "def modify_with_lora(transformer, config):\n",
    "    for m_name, module in dict(transformer.named_modules()).items():\n",
    "        if re.fullmatch(config.lora_modules, m_name):\n",
    "            for c_name, layer in dict(module.named_children()).items():\n",
    "                if re.fullmatch(config.lora_layers, c_name):\n",
    "                    if isinstance(layer, nn.Linear):\n",
    "                        setattr(module, c_name,\n",
    "                            LoRALinear(layer, config.lora_rank, config.lora_scaling_rank, config.lora_init_scale))\n",
    "    return transformer\n",
    "\n",
    "class ClassConfig:\n",
    "    def __init__(self, dropout=0.2, num_labels=1):\n",
    "        self.dropout_rate = dropout\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "class T5EncoderClassificationHead(nn.Module):\n",
    "    def __init__(self, hidden_size, class_config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(hidden_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(class_config.dropout_rate)\n",
    "        self.out_proj = nn.Linear(hidden_size, class_config.num_labels)\n",
    "    def forward(self, hidden_states):\n",
    "        pooled = hidden_states[:, 0, :]  # first token ([CLS]-like)\n",
    "        pooled = self.dropout(pooled)\n",
    "        pooled = self.dense(pooled)\n",
    "        pooled = torch.tanh(pooled)\n",
    "        pooled = self.dropout(pooled)\n",
    "        logits = self.out_proj(pooled)\n",
    "        return logits\n",
    "\n",
    "class T5EncoderForSimpleSequenceRegression(T5PreTrainedModel):\n",
    "    def __init__(self, encoder_model, config: T5Config, class_config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = class_config.num_labels\n",
    "        self.config = config\n",
    "        self.encoder_model = encoder_model\n",
    "        self.dropout = nn.Dropout(class_config.dropout_rate)\n",
    "        self.classifier = T5EncoderClassificationHead(config.d_model, class_config)\n",
    "        self.post_init()\n",
    "        self.model_parallel = False\n",
    "        self.device_map = None\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        outputs = self.encoder_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            head_mask=head_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "        logits = self.classifier(hidden_states)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = MSELoss()\n",
    "            loss = loss_fct(logits.squeeze(), labels.squeeze())\n",
    "        if not return_dict:\n",
    "            output = (logits,) + (outputs[1:] if isinstance(outputs, tuple) else ())\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=getattr(outputs, \"hidden_states\", None),\n",
    "            attentions=getattr(outputs, \"attentions\", None),\n",
    "        )\n",
    "\n",
    "def PT5_regression_model(num_labels=1, half_precision=False):\n",
    "    encoder_model = T5EncoderModel.from_pretrained(\"Rostlab/prot_t5_xl_uniref50\")\n",
    "    tokenizer = T5Tokenizer.from_pretrained(\"Rostlab/prot_t5_xl_uniref50\", legacy=False)\n",
    "    class_config = ClassConfig(num_labels=num_labels)\n",
    "    class_model = T5EncoderForSimpleSequenceRegression(encoder_model, encoder_model.config, class_config)\n",
    "    config = LoRAConfig()\n",
    "    class_model.encoder_model = modify_with_lora(class_model.encoder_model, config)\n",
    "    for name, param in class_model.named_parameters():\n",
    "        if re.fullmatch(config.trainable_param_names, name):\n",
    "            param.requires_grad = True\n",
    "        else:\n",
    "            param.requires_grad = False\n",
    "    return class_model, tokenizer\n",
    "\n",
    "def set_seeds(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    set_seed(seed)\n",
    "\n",
    "def create_dataset(tokenizer, seqs, labels, weights=None, max_length=128):\n",
    "    seqs_spaced = [s for s in seqs]\n",
    "    tokenized = tokenizer(seqs_spaced, max_length=max_length, padding='max_length', truncation=True)\n",
    "    dataset = Dataset.from_dict(tokenized)\n",
    "    dataset = dataset.add_column(\"labels\", labels)\n",
    "    if weights is not None:\n",
    "        dataset = dataset.add_column(\"weights\", weights)\n",
    "    return dataset\n",
    "\n",
    "def train_per_protein(\n",
    "    train_df, \n",
    "    valid_df, \n",
    "    num_labels=1, \n",
    "    batch=4,\n",
    "    accum=1,\n",
    "    val_batch=16,\n",
    "    epochs=10,\n",
    "    lr=2e-4,\n",
    "    wd=0.01,\n",
    "    seed=42, \n",
    "    mixed=False,\n",
    "):\n",
    "    set_seeds(seed)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model, tokenizer = PT5_regression_model(num_labels=num_labels, half_precision=mixed)\n",
    "    model.to(device)\n",
    "    train_set = create_dataset(tokenizer, list(train_df[\"sequence\"]), list(train_df[\"fitness\"]), list(train_df[\"weight\"]), max_length=128)\n",
    "    valid_set = create_dataset(tokenizer, list(valid_df[\"sequence\"]), list(valid_df[\"fitness\"]), list(valid_df[\"weight\"]), max_length=128)\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        output_dir=\"./outputs\",\n",
    "        do_train=True,\n",
    "        do_eval=True,\n",
    "        per_device_train_batch_size=batch,\n",
    "        per_device_eval_batch_size=val_batch,\n",
    "        gradient_accumulation_steps=accum,\n",
    "        num_train_epochs=epochs,\n",
    "        weight_decay=wd,\n",
    "        learning_rate=lr,\n",
    "        seed=seed,\n",
    "        fp16=False,\n",
    "        dataloader_num_workers=2,\n",
    "        report_to=\"none\",\n",
    "        save_total_limit=1,\n",
    "    )\n",
    "    def compute_metrics(eval_pred):\n",
    "        predictions, labels = eval_pred\n",
    "        predictions = np.squeeze(predictions)\n",
    "        labels = np.squeeze(labels)\n",
    "        correlation = stats.spearmanr(predictions, labels).correlation\n",
    "        if np.isnan(correlation):\n",
    "            correlation = 0.0\n",
    "        return {\"spearmanr\": correlation}\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_set,\n",
    "        eval_dataset=valid_set,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    print(\"Starting training...\")\n",
    "    trainer.train()\n",
    "    print(\"Training completed!\")\n",
    "    return tokenizer, model, trainer.state.log_history\n",
    "\n",
    "# ======================== 7. Training Loop (Simple) ==========================\n",
    "print(\"Starting PT5 regression training...\")\n",
    "\n",
    "tokenizer, model, history = train_per_protein(\n",
    "    my_train,\n",
    "    my_valid,\n",
    "    num_labels=1,\n",
    "    batch=4,\n",
    "    accum=1,\n",
    "    val_batch=16,\n",
    "    epochs=10,\n",
    "    lr=2e-4,\n",
    "    wd=0.01,\n",
    "    seed=42,\n",
    "    mixed=False,\n",
    ")\n",
    "\n",
    "# ======================== 8. Plot Results ==========================\n",
    "loss = [x['loss'] for x in history if 'loss' in x]\n",
    "val_loss = [x['eval_loss'] for x in history if 'eval_loss' in x]\n",
    "metric = [x['eval_spearmanr'] for x in history if 'eval_spearmanr' in x]\n",
    "epochs_list = [x['epoch'] for x in history if 'loss' in x]\n",
    "\n",
    "if len(loss) > 0:\n",
    "    fig, ax1 = plt.subplots(figsize=(8, 4))\n",
    "    ax2 = ax1.twinx()\n",
    "    line1 = ax1.plot(epochs_list, loss, label='train_loss')\n",
    "    line2 = ax1.plot(epochs_list, val_loss, label='val_loss') if len(val_loss) > 0 else []\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    line3 = ax2.plot(epochs_list, metric, color='red', label='val_metric') if len(metric) > 0 else []\n",
    "    ax2.set_ylabel('Spearman r')\n",
    "    ax2.set_ylim([0, 1])\n",
    "    lines = line1 + line2 + line3\n",
    "    labels = [line.get_label() for line in lines]\n",
    "    ax1.legend(lines, labels, loc='lower left')\n",
    "    plt.title(\"Training History\")\n",
    "    plt.show()\n",
    "\n",
    "# ====================== 9. Save & Reload Model (LoRA weights only) =========================\n",
    "def save_model(model, filepath):\n",
    "    non_frozen_params = {n: p for n, p in model.named_parameters() if p.requires_grad}\n",
    "    torch.save(non_frozen_params, filepath)\n",
    "    print(f\"Model saved to {filepath}\")\n",
    "\n",
    "def load_model(filepath, num_labels=1, mixed=False):\n",
    "    model, tokenizer = PT5_regression_model(num_labels=num_labels, half_precision=mixed)\n",
    "    non_frozen_params = torch.load(filepath, map_location='cpu')\n",
    "    for name, param in model.named_parameters():\n",
    "        if name in non_frozen_params:\n",
    "            param.data = non_frozen_params[name].data.clone()\n",
    "    print(f\"Model loaded from {filepath}\")\n",
    "    return tokenizer, model\n",
    "\n",
    "save_model(model, \"./PT5_finetuned.pth\")\n",
    "tokenizer_reload, model_reload = load_model(\"./PT5_finetuned.pth\", num_labels=1, mixed=False)\n",
    "\n",
    "# ======================== 10. Inference on Test Set ==========================\n",
    "print(\"Running inference on test set...\")\n",
    "\n",
    "test_set = create_dataset(tokenizer_reload, list(my_test[\"sequence\"]), list(my_test[\"fitness\"]), list(my_test[\"weight\"]), max_length=128)\n",
    "test_set = test_set.with_format(\"torch\")\n",
    "test_dataloader = DataLoader(test_set, batch_size=8, shuffle=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_reload.to(device)\n",
    "model_reload.eval()\n",
    "\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_dataloader, desc=\"Testing\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        logits = model_reload(input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "        batch_predictions = logits.cpu().numpy().squeeze()\n",
    "        if batch_predictions.ndim == 0:\n",
    "            batch_predictions = [batch_predictions.item()]\n",
    "        else:\n",
    "            batch_predictions = batch_predictions.tolist()\n",
    "        predictions.extend(batch_predictions)\n",
    "\n",
    "true_labels = my_test['fitness'].values\n",
    "predictions_arr = np.array(predictions)\n",
    "test_correlation = stats.spearmanr(predictions_arr, true_labels).correlation\n",
    "pearson_corr = stats.pearsonr(predictions_arr, true_labels)[0]\n",
    "\n",
    "print(f\"Test Spearman r: {test_correlation:.4f}\")\n",
    "print(f\"Test Pearson r:  {pearson_corr:.4f}\")\n",
    "print(f\"Variance of predictions: {np.var(predictions_arr):.6f}\")\n",
    "\n",
    "print(\"\\nTrue vs Predicted on Test Set:\")\n",
    "print(pd.DataFrame({\n",
    "    \"True\": true_labels,\n",
    "    \"Pred\": predictions_arr\n",
    "}).head(20))\n",
    "\n",
    "# ======================== 11. Plot Test Predictions ==========================\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.scatterplot(x=true_labels, y=predictions_arr, s=80, color='royalblue', edgecolor='k')\n",
    "plt.plot([true_labels.min(), true_labels.max()],\n",
    "         [true_labels.min(), true_labels.max()],\n",
    "         'r--', lw=2, label='Identity (y=x)')\n",
    "plt.xlabel(\"True Fitness (Label)\")\n",
    "plt.ylabel(\"Predicted Fitness\")\n",
    "plt.title(f\"Test Set Predictions vs True (Spearman r = {test_correlation:.3f})\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
