{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19804,
     "status": "ok",
     "timestamp": 1742119672683,
     "user": {
      "displayName": "Ryoji Takahashi",
      "userId": "08099237406056068712"
     },
     "user_tz": -60
    },
    "id": "7TfzYEa2qLz4",
    "outputId": "47496dfb-898c-434a-d57d-3f9974f7d90a"
   },
   "outputs": [],
   "source": [
    "# Install necessary packages\n",
    "!pip install numpy pandas pyjanitor jax_unirep scikit-learn matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UhZ2VAAC7r20"
   },
   "outputs": [],
   "source": [
    "# Install necessary tools in Google Colab\n",
    "!apt-get install -y hmmer\n",
    "!wget https://github.com/soedinglab/MMseqs2/releases/download/17-b804f/mmseqs-linux-gpu.tar.gz\n",
    "!tar xvf mmseqs-linux-gpu.tar.gz\n",
    "!chmod +x mmseqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "xGLKt38Kta0W",
    "outputId": "ec064206-2678-4b79-943a-cf680e55a576"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import time\n",
    "import subprocess\n",
    "import os\n",
    "import janitor\n",
    "from functools import partial\n",
    "from jax_unirep import get_reps\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# URLs for external tools\n",
    "BLAST_URL = \"https://blast.ncbi.nlm.nih.gov/Blast.cgi\"\n",
    "\n",
    "# Wild-type enzyme sequence\n",
    "WT_SEQUENCE = \"MRHGDISSSNDTVGVAVVNYKMPRLHTAAEVLDNARKIAEMIVGMKQGLPGMDLVVFPEYSLQGIMYDPAEMMETAVAIPGEETEIFSRACRKANVWGVFSLTGERHEEHPRKAPYNTLVLIDNNGEIVQKYRKIIPWCPIEGWYPGGQTYVSEGPKGMKISLIICDDGNYPEIWRDCAMKGAELIVRCQGYMYPAKDQQVMMAKAMAWANNCYVAVANAAGFDGVYSYFGHSAIIGFDGRTLGECGEEEMGIQYAQLSLSQIRDARANDQSQNHLFKILHRGYSGLQASGDGDRGLAECPFEFYRTWVTDAEKARENVERLTRSTTGVAQCPVGRLPYEG\"\n",
    "\n",
    "\n",
    "def run_blast_search(wt_sequence, identity_threshold=98.0, max_retries=20, sleep_time=10):\n",
    "    \"\"\"Run BLAST search and extract homologous sequences below a given identity threshold.\"\"\"\n",
    "\n",
    "    # **1. Submit BLAST Query**\n",
    "    params = {\n",
    "        \"CMD\": \"Put\",\n",
    "        \"PROGRAM\": \"blastp\",\n",
    "        \"DATABASE\": \"nr\",\n",
    "        \"QUERY\": wt_sequence,\n",
    "        \"FORMAT_TYPE\": \"XML\",\n",
    "        \"EXPECT\": \"10\",  # Set e-value threshold (adjustable)\n",
    "        \"HITLIST_SIZE\": \"500\"  # Increase number of results\n",
    "    }\n",
    "\n",
    "    response = requests.post(BLAST_URL, data=params)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Error submitting sequence: {response.text}\")\n",
    "\n",
    "    response_text = response.text\n",
    "    if \"RID = \" not in response_text:\n",
    "        raise Exception(\"Error: No RID found in response.\")\n",
    "\n",
    "    rid = response_text.split(\"RID = \")[-1].split(\"\\n\")[0].strip()\n",
    "    print(f\" Submitted BLAST job with RID: {rid}\")\n",
    "\n",
    "    # **2. Wait for BLAST results**\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "        status_params = {\"CMD\": \"Get\", \"FORMAT_OBJECT\": \"SearchInfo\", \"RID\": rid}\n",
    "        status_response = requests.get(BLAST_URL, params=status_params)\n",
    "\n",
    "        if \"Status=READY\" in status_response.text:\n",
    "            print(\" BLAST search is complete.\")\n",
    "            break\n",
    "        elif \"Status=FAILED\" in status_response.text:\n",
    "            raise Exception(\" BLAST job failed on NCBI server.\")\n",
    "\n",
    "        print(f\"⏳ Waiting for BLAST results... (Attempt {retries + 1}/{max_retries})\")\n",
    "        time.sleep(sleep_time)\n",
    "        retries += 1\n",
    "\n",
    "    if retries == max_retries:\n",
    "        raise Exception(\" BLAST API timed out. Try increasing max_retries or sleep_time.\")\n",
    "\n",
    "    # **3. Retrieve BLAST results**\n",
    "    result_params = {\"CMD\": \"Get\", \"FORMAT_TYPE\": \"XML\", \"RID\": rid}\n",
    "    result_response = requests.get(BLAST_URL, params=result_params)\n",
    "    if result_response.status_code != 200:\n",
    "        raise Exception(f\"Error retrieving results: {result_response.text}\")\n",
    "\n",
    "    # Print raw response for debugging\n",
    "    print(result_response.text[:1000])  # Print first 1000 characters to check if XML is valid\n",
    "\n",
    "    # **4. Parse BLAST results**\n",
    "    root = ET.fromstring(result_response.text)\n",
    "    homologous_sequences = []\n",
    "    query_sequences = []\n",
    "    identities = []\n",
    "\n",
    "    for hit in root.findall(\".//Hit\"):\n",
    "        hit_id = hit.find(\"Hit_id\").text if hit.find(\"Hit_id\") is not None else \"Unknown\"\n",
    "        hit_def = hit.find(\"Hit_def\").text if hit.find(\"Hit_def\") is not None else \"Unknown\"\n",
    "\n",
    "        for hsp in hit.findall(\".//Hsp\"):\n",
    "            hsp_qseq_elem = hsp.find(\"Hsp_qseq\")\n",
    "            hsp_hseq_elem = hsp.find(\"Hsp_hseq\")\n",
    "            identity_elem = hsp.find(\"Hsp_identity\")\n",
    "            align_len_elem = hsp.find(\"Hsp_align-len\")\n",
    "\n",
    "            if hsp_qseq_elem is None or hsp_hseq_elem is None or identity_elem is None or align_len_elem is None:\n",
    "                continue  # Skip if missing data\n",
    "\n",
    "            qseq = hsp_qseq_elem.text.strip()\n",
    "            hseq = hsp_hseq_elem.text.strip()\n",
    "            identity = int(identity_elem.text)\n",
    "            align_len = int(align_len_elem.text)\n",
    "            identity_percentage = (identity / align_len) * 100\n",
    "\n",
    "            if identity_percentage <= identity_threshold:\n",
    "                homologous_sequences.append(hseq)  # Homologous sequences\n",
    "                query_sequences.append(qseq)  # Query sequences\n",
    "                identities.append(identity_percentage)\n",
    "                print(f\"✅ Found sequence from {hit_id}: {hit_def} (Identity: {identity_percentage:.2f}%)\")\n",
    "\n",
    "    print(f\" Total homologous sequences collected: {len(homologous_sequences)}\")\n",
    "    print(f\" Total query sequences collected: {len(query_sequences)}\")\n",
    "\n",
    "    return homologous_sequences, query_sequences, identities\n",
    "\n",
    "# **Run BLAST API Search**\n",
    "homologous_sequences, query_sequences, identities = run_blast_search(WT_SEQUENCE)\n",
    "\n",
    "print(f\" Retrieved {len(homologous_sequences)} homologous sequences.\")\n",
    "print(f\" Retrieved {len(query_sequences)} query sequences.\")\n",
    "print(f\" Average Sequence Identity: {sum(identities)/len(identities) if identities else 0:.2f}%\")\n",
    "\n",
    "\n",
    "def run_jackhmmer_search(wt_sequence, num_iter=3, evalue=1e-5):\n",
    "    \"\"\"Run Jackhmmer search using HMMER locally. Assumes HMMER is installed.\"\"\"\n",
    "    fasta_filename = \"wt_sequence.fasta\"\n",
    "    with open(fasta_filename, \"w\") as f:\n",
    "        f.write(f\">WT_SEQUENCE\\n{wt_sequence}\\n\")\n",
    "\n",
    "    output_file = \"jackhmmer_output.txt\"\n",
    "    cmd = f\"jackhmmer --tblout {output_file} -N {num_iter} --cpu 4 -E {evalue} {fasta_filename} /usr/share/uniprot_sprot.fasta\"\n",
    "\n",
    "    subprocess.run(cmd, shell=True, check=True)\n",
    "\n",
    "    sequences = []\n",
    "    with open(output_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            if not line.startswith(\"#\"):\n",
    "                columns = line.strip().split()\n",
    "                if len(columns) > 1:\n",
    "                    sequences.append(columns[0])  # Extract sequence ID\n",
    "\n",
    "    print(f\"Total sequences collected from Jackhmmer: {len(sequences)}\")\n",
    "    return sequences if sequences else [wt_sequence]\n",
    "\n",
    "\n",
    "def run_mmseqs2_search(wt_sequence):\n",
    "    \"\"\"Run MMseqs2 locally in Google Colab.\"\"\"\n",
    "\n",
    "    # Save WT sequence to FASTA file\n",
    "    fasta_filename = \"wt_sequence.fasta\"\n",
    "    with open(fasta_filename, \"w\") as f:\n",
    "        f.write(f\">WT_SEQUENCE\\n{wt_sequence}\\n\")\n",
    "\n",
    "    db_name = \"uniref50\"\n",
    "    output_file = \"mmseqs_output.m8\"\n",
    "\n",
    "    # Create database, search, and convert output\n",
    "    subprocess.run(f\"mmseqs createdb {fasta_filename} wt_sequence_db\", shell=True, check=True)\n",
    "    subprocess.run(f\"mmseqs search wt_sequence_db {db_name} results tmp --threads 4\", shell=True, check=True)\n",
    "    subprocess.run(f\"mmseqs convertalis wt_sequence_db {db_name} results {output_file}\", shell=True, check=True)\n",
    "\n",
    "    sequences = []\n",
    "    with open(output_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            columns = line.strip().split(\"\\t\")\n",
    "            if len(columns) > 1:\n",
    "                sequences.append(columns[1])  # Extract sequence ID\n",
    "\n",
    "    print(f\"Total sequences collected from MMseqs2: {len(sequences)}\")\n",
    "    return sequences if sequences else [wt_sequence]\n",
    "\n",
    "\n",
    "# Example Usage\n",
    "method = \"blast\"  # Change to \"jackhmmer\" or \"mmseqs2\" as needed\n",
    "\n",
    "if method == \"blast\":\n",
    "    homologous_sequences = run_blast_search(WT_SEQUENCE)\n",
    "elif method == \"jackhmmer\":\n",
    "    homologous_sequences = run_jackhmmer_search(WT_SEQUENCE)\n",
    "elif method == \"mmseqs2\":\n",
    "    homologous_sequences = run_mmseqs2_search(WT_SEQUENCE)\n",
    "else:\n",
    "    raise ValueError(\"Invalid method chosen. Please select 'blast', 'jackhmmer', or 'mmseqs2'.\")\n",
    "\n",
    "print(f\"Retrieved {len(homologous_sequences)} sequences using {method}.\")\n",
    "\n",
    "def generate_mutated_sequence(wt_sequence, mutation_string):\n",
    "    \"\"\"Generate mutated sequence based on mutation string.\"\"\"\n",
    "    seq_list = list(wt_sequence)\n",
    "    mutations = mutation_string.split(',')\n",
    "    for mut in mutations:\n",
    "        if len(mut) >= 3 and mut[1:-1].isdigit():\n",
    "            pos = int(mut[1:-1]) - 1  # Convert to 0-based index\n",
    "            if pos < len(seq_list) and mut[-1] != '*':\n",
    "                seq_list[pos] = mut[-1]\n",
    "    return ''.join(seq_list)\n",
    "\n",
    "# Load dataset from Figshare\n",
    "url = \"https://figshare.com/ndownloader/files/7337543\"\n",
    "df = pd.read_csv(url, sep='\\t')\n",
    "\n",
    "df.rename(columns={'mutation': 'mutation_string', 'normalized_fitness': 'fitness'}, inplace=True)\n",
    "\n",
    "# Check for invalid values in 'fitness' column (e.g., 'NS', NaN, etc.)\n",
    "df['fitness'] = pd.to_numeric(df['fitness'], errors='coerce')  # Convert invalid values to NaN\n",
    "df.dropna(subset=['fitness'], inplace=True)  # Drop rows with NaN values in 'fitness'\n",
    "\n",
    "df['mutated_sequence'] = df['mutation_string'].apply(lambda x: generate_mutated_sequence(WT_SEQUENCE, x))\n",
    "\n",
    "def chunker(seq, size):\n",
    "    return (seq[pos:pos + size] for pos in range(0, len(seq), size))\n",
    "\n",
    "h_final_l, c_final_l, h_avg_l = [], [], []\n",
    "for seqs in chunker(df.mutated_sequence.values, 1000):\n",
    "    hf, cf, ha = get_reps(seqs)\n",
    "    h_final_l.append(hf)\n",
    "    c_final_l.append(cf)\n",
    "    h_avg_l.append(ha)\n",
    "\n",
    "h_final = np.concatenate(h_final_l, axis=0)\n",
    "c_final = np.concatenate(c_final_l, axis=0)\n",
    "h_avg = np.concatenate(h_avg_l, axis=0)\n",
    "\n",
    "df[\"h_avg\"] = h_avg.tolist()\n",
    "df[\"h_final\"] = h_final.tolist()\n",
    "df[\"c_final\"] = c_final.tolist()\n",
    "\n",
    "def fusion(x):\n",
    "    return np.concatenate((x.h_final, x.c_final, x.h_avg))\n",
    "\n",
    "df = df.join_apply(fusion, \"unirep_fusion\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(np.vstack(df[\"unirep_fusion\"].values))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, df[\"fitness\"].values, test_size=0.2, random_state=42)\n",
    "\n",
    "param_grid_ridge = {'alpha': [0.01, 0.1, 1, 10, 100]}\n",
    "param_grid_lasso = {'alpha': [0.01, 0.1, 1, 10, 100]}\n",
    "param_grid_en = {'alpha': [0.01, 0.1, 1, 10, 100], 'l1_ratio': [0.1, 0.5, 0.9, 1.0]}\n",
    "\n",
    "ridge_model = Ridge()\n",
    "lasso_model = Lasso()\n",
    "elastic_net_model = ElasticNet()\n",
    "\n",
    "def train_and_evaluate_model(model, param_grid, model_name):\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    y_pred = grid_search.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    pearson_corr, _ = pearsonr(y_test, y_pred)\n",
    "    print(f\"{model_name} - MSE: {mse:.4f}, Pearson Corr: {pearson_corr:.4f}\")\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(y_test, y_pred, alpha=0.6)\n",
    "    plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], 'r--')\n",
    "    plt.xlabel('Actual Fitness')\n",
    "    plt.ylabel('Predicted Fitness')\n",
    "    plt.title(f'{model_name} Prediction')\n",
    "    plt.show()\n",
    "\n",
    "train_and_evaluate_model(ridge_model, param_grid_ridge, \"Ridge\")\n",
    "train_and_evaluate_model(lasso_model, param_grid_lasso, \"Lasso\")\n",
    "train_and_evaluate_model(elastic_net_model, param_grid_en, \"ElasticNet\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMybUQCW/nDKTrZ3nzDVMPH",
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "13g0f0QV7_b8z20vL2w-raBrhkWr37gij",
     "timestamp": 1742064689201
    },
    {
     "file_id": "16j2Q7sOIgDyOGhmLXBJcgCE2pcfXUOhS",
     "timestamp": 1741975886549
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
